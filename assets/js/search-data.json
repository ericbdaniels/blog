{
  
    
  
    
        "post1": {
            "title": "Getting Started with Python on Windows",
            "content": "Getting Started with Python on Windows . Reading python how-to guides, blogs and even tweets often gives the illusion that all things python take place entirely on Linux and Mac OS. I’m on windows 99% of the time when using python and happy to report it works great. There is a bit of a lingering stigma regarding python on windows because there was a time when getting python installed and setup on a windows machine was a quite a hassle. Those days are over. In fact, there are more windows users than mac users according to the JetBrains survey of 24k python users: . . Good news, that is absolutely no longer the case. However, I do think getting started with Python on windows can be a bit confusing. The goal of this post is to give a brief, but opinionated summary of how to go from never having touched python to running code in a jupyter notebook ASAP. . Where to get Python . Google something along the lines of “install python windows” and you’ll get python.org, closely followed by realpython.com and then a Microsoft page suggesting a python download from the Microsoft Appstore. Instead of these options, seek out Anaconda. This is a cross-platform solution to getting python installed and running with ease. . The Anaconda distribution includes: . The latest version of python (create an environment for other versions needed - more on that to come) | conda command line tool | A substantial set of pre-installed packages (particularly useful for scientific computing or data science) | Anaconda Navigator (for those that prefer a GUI to launch python based tools) | . Note: that if you desire a lighter weight version of Anaconda without the pre-installed packages consider miniconda . Starting Jupyter . Jupyter Notebooks and Jupyter Lab are an approachable way to get started writing some code for beginners, as well as a fantastic way to combine data visualization with text and blocks of code for more experienced users. . To get started either launch anaconda navigator from the start menu, or open a command line terminal, navigate to your working directory and open by either jupyter notebookor jupyter lab. . For some the command line terminal is unfamiliar or intimidating - instead jupyter notebook or lab can be launched from the file explorer with the same command (juptyer notebook or jupter lab) entered in the address bar: . . Virtual Environments with . A virtual environment can be thought of as an isolated install of python with a set of packages separate from the system (or user) -wide install. A virtual environment provides the freedom to work on projects that require a different version of python, or necessitate package(s) of different versions despite the latest and greatest that was installed with the Anaconda download. Additionally, a virtual environment can be replicated in another location or on another machine ensuring the ability to run your code wherever its needed. Shared below are some common conda commands for creating/using/cloning environments. If you need more info the conda documentation is excellent. . Environment Setup . Creating a new environment is simple . conda env create -n mynewenv (replacing mynewenv with a more appropriate name). . Anytime you want to use this environment, activate it . conda activate mynewenv . Once activated, anytime you call python or it will call the version specific to that environment, isolated from the global install. . Note: Using Jupyter (Notebook or Lab) within a Conda Environment requires an extra step. . If you want to use your conda environment in a jupyter notebook : . conda install ipykernel nb_conda_kernels . ipykernel provides everything you need to run jupyter from your new environment | nb_conda_kernels provides the added bonus of being able to access other conda environments on your system from within jupyter | . Copying an Environment . Ultimately, you’ll want to share your code with a friend or colleague to replicate some python workflow on a new machine or new data elsewhere. Using a Virtual Environment makes this easy. To replicate an environment, simply record all packages being used and their version number. To clone an environment on the same machine: . conda create --name myenvclone --clone myoldenv . Conda also provides a convenient way to export all the pertinent information to a YAML file. From within the environment you’d like to copy: . conda env export &gt; environment.yml . Now, take this environment.yml and recreate that exact same environment on a different machine: . conda env create -f environment.yml . Note: The YAML approach for saving environment information does not replace the standard requirements.txt that is common with pip. Instead, the conda approach is best suited for exactly the process described above: cloning an environment. Same name, all the same packages etc. requirements.txt is best suited for a list dependencies required for some package, stopping short of creating an entire environment. . Conclusion . Python works great on windows, don’t let anyone tell you otherwise. If you’re working on windows, especially in the realm of scientific computing or data science, Anaconda is highly recommended. Once you’ve got Anaconda set up and are comfortable with the basics, consider working a virtual environment for most things you’re doing. Does absolutely every bit of code need its own environment, definitely not but for any substantial project its highly recommended. .",
            "url": "geostats.dev/beginner/python/anaconda/conda/virtual%20environments/2020/10/08/python-on-windows.html",
            "relUrl": "/beginner/python/anaconda/conda/virtual%20environments/2020/10/08/python-on-windows.html",
            "date": " • Oct 8, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Pandas DataFrames with a GSLIB I/O Methods",
            "content": "Pandas is everywhere, for good reason. If you are using python to manipulate data, chances are you&#39;re using pandas. If you are using python for geoscience, you&#39;ve likely come across some weird FORTRAN generated file format that was a great idea in the 80s, but is a bit of a hassle for I/O operations. Pandas offers a large number of read/write methods, but occasionally some archaic file format comes along that can be a challenge (MODFLOW anyone?). But don&#39;t limit yourself to I/O operations, if there is additional functionality you desire from Pandas, you can extend dataframe, series or index functionality to do just that. If extending existing Pandas objects isn&#39;t enough, with a little extra effort and a few important details subclass and make your own DataFrame class. . Step 1: Reusable Function . In the last post about reading/writing GSLIB files, I shared a couple short snippets I use for reading/writing GEO-EAS (GSLIB) files to/from Pandas. As an example, I&#39;ll extend pandas to include these convenient read/write operations: . def write_gslib(self, filename:str): with open(filename, &quot;w&quot;) as f: f.write(&quot;GSLIB Example Data n&quot;) f.write(f&quot;{len(self._obj.columns)} n&quot;) f.write(&quot; n&quot;.join(self._obj.columns)+&quot; n&quot;) for row in df.itertuples(): row_data = &quot; t&quot;.join([f&quot;{i:.3f}&quot; for i in row[1:]]) f.write(f&quot;{row_data} n&quot;) def read_gslib(filename:str): with open(filename, &quot;r&quot;) as f: lines = f.readlines() ncols = int(lines[1].split()[0]) col_names = [lines[i+2].strip() for i in range(ncols)] df = pd.read_csv(filename, skiprows=ncols+2, delim_whitespace=True, names=col_names) return df . Step 2: pd.DataFrame Accessor . I&#39;m a big fan of this Pandas functionality. Just a simple decorator opens up the ability to add your own methods, properties etc. Here are the steps: . Make a class out your function(s). I&#39;ll call mine . GSLIBAccessor: . | class GSLIBAccessor: def __init__(self, pandas_obj): self._obj=(pandas_obj) def write_gslib(self, filename:str): with open(filename, &quot;w&quot;) as f: f.write(&quot;GSLIB Example Data n&quot;) f.write(f&quot;{len(self._obj.columns)} n&quot;) f.write(&quot; n&quot;.join(self._obj.columns)+&quot; n&quot;) for row in df.itertuples(): row_data = &quot; t&quot;.join([f&quot;{i:.3f}&quot; for i in row[1:]]) f.write(f&quot;{row_data} n&quot;) def read_gslib(filename:str): with open(filename, &quot;r&quot;) as f: lines = f.readlines() ncols = int(lines[1].split()[0]) col_names = [lines[i+2].strip() for i in range(ncols)] df = pd.read_csv(filename, skiprows=ncols+2, delim_whitespace=True, names=col_names) return df . In the __init__ include your DataFrame that this method will be operating on, pandas_obj in above snippet. | Add the decorator (Also I removed the _gslib suffix): | @pd.api.extensions.register_dataframe_accessor(&quot;gslib&quot;) class GSLIBAccessor: def __init__(self, pandas_obj): self._obj=(pandas_obj) def write(self, filename:str): with open(filename, &quot;w&quot;) as f: f.write(&quot;GSLIB Example Data n&quot;) f.write(f&quot;{len(self._obj.columns)} n&quot;) f.write(&quot; n&quot;.join(self._obj.columns)+&quot; n&quot;) for row in df.itertuples(): row_data = &quot; t&quot;.join([f&quot;{i:.3f}&quot; for i in row[1:]]) f.write(f&quot;{row_data} n&quot;) @staticmethod def read(filename:str): with open(filename, &quot;r&quot;) as f: lines = f.readlines() ncols = int(lines[1].split()[0]) col_names = [lines[i+2].strip() for i in range(ncols)] df = pd.read_csv(filename, skiprows=ncols+2, delim_whitespace=True, names=col_names) return df . Boom! Done! Thats it. Fantastic, right? . Now, the functionwrite_gslib is available as a DataFrame method at df.gslib.write() . There are plenty of GSLIB specific details to manage here - fill null values with -999.00, add grid definition in the file header etc, but the point here is this is a fast, easy, flexible way to add whatever functionality you need to the DataFrame class. . Reading in a dataframe by this approach works just fine as well. Keep in mind, the method created is associated with the DataFrame and won&#39;t be accessible at same place as the other pandas I/O operations like pd.read_csv, instead it will be at pd.DataFrame.gslib.read. . df = pd.DataFrame.gslib.read(&quot;data/example.dat&quot;) df.head() . x y z var . 0 0.723 | 0.564 | 0.785 | 2.853 | . 1 0.915 | 0.317 | 0.357 | 0.749 | . 2 0.346 | 0.484 | 0.690 | 0.786 | . 3 0.591 | 0.150 | 0.669 | 0.290 | . 4 0.157 | 0.332 | 0.006 | 1.777 | . To write out the file: . df.gslib.write(&quot;data/export_data.dat&quot;) . with open(&quot;data/export_data.dat&quot;, &quot;r&quot;) as f: for i in range(10): print(f.readline().strip()) . GSLIB Example Data 4 x y z var 0.723 0.564 0.785 2.853 0.915 0.317 0.357 0.749 0.346 0.484 0.690 0.786 0.591 0.150 0.669 0.290 . Step 3: Subclassing Pandas DataFrame . If forwhatever reason the decorator accessor approach isn&#39;t enough, you can always create your own class entirely and inheirit pd.DataFrame. This is a bit more work and there are a couple import details not to be missed. . Inheiritance . Inheiritance is a common aspect of OOP (object oriented programming), and is a topic that warrants a discussion all its own. Rather than get into that can of worms, if want some more details I&#39;d suggest Real Python: Inheiritance and Composition. In example, create a new class and inheirit all the good things that pd.DataFrame does, but add a few properities, methods etc. . class GSLIBDataFrame(pd.DataFrame): def __init__(self, data, *args, **kwargs): super().__init__(data=data, *args, **kwargs) def write(self, filename:str): with open(filename, &quot;w&quot;) as f: f.write(&quot;GSLIB Example Data n&quot;) f.write(f&quot;{len(self._obj.columns)} n&quot;) f.write(&quot; n&quot;.join(self._obj.columns)+&quot; n&quot;) for row in df.itertuples(): row_data = &quot; t&quot;.join([f&quot;{i:.3f}&quot; for i in row[1:]]) f.write(f&quot;{row_data} n&quot;) @classmethod def read(cls, filename:str): with open(filename, &quot;r&quot;) as f: lines = f.readlines() ncols = int(lines[1].split()[0]) col_names = [lines[i+2].strip() for i in range(ncols)] df = pd.read_csv(filename, skiprows=ncols+2, delim_whitespace=True, names=col_names) return cls(df) . This works and is quite similar to the accessor example but I do think this apporach would scale better should you have big plans for your DIY dataframes. Lets consider a couple issues shown below . df = GSLIBDataFrame.read(&quot;data/example.dat&quot;) returned_df = df.applymap(lambda x: x*2) type(returned_df) . pandas.core.frame.DataFrame . If you use some standard pandas operatione, it will return a regular pd.DataFrame not a GSLIBDataFrame - To manage this, the _constructor must be defined to override the method inheirted from pandas. . | Any other properites created, but be added to the metadata list so that they are passed on to results of manipulation. . | To demonstrate in addition to the previously defined methods, add a property favorite_column, though this name is nonsense this can be a useful approach for defining a specific column that defines categories, domains or coordinates. . class GSLIBDataFrame(pd.DataFrame): def __init__(self, data, favorite_column=None, *args, **kwargs): super().__init__(data=data, *args, **kwargs) self.favorite_column = favorite_column _metadata = [&quot;favorite_column&quot;] @property def _constructor(self): return GSLIBDataFrame def write(self, filename:str): with open(filename, &quot;w&quot;) as f: f.write(&quot;GSLIB Example Data n&quot;) f.write(f&quot;{len(self._obj.columns)} n&quot;) f.write(&quot; n&quot;.join(self._obj.columns)+&quot; n&quot;) for row in df.itertuples(): row_data = &quot; t&quot;.join([f&quot;{i:.3f}&quot; for i in row[1:]]) f.write(f&quot;{row_data} n&quot;) @classmethod def read(cls, filename:str, favorite_column:str): with open(filename, &quot;r&quot;) as f: lines = f.readlines() ncols = int(lines[1].split()[0]) col_names = [lines[i+2].strip() for i in range(ncols)] df = pd.read_csv(filename, skiprows=ncols+2, delim_whitespace=True, names=col_names) return cls(data=df, favorite_column=favorite_column) . Now, instead of returning a pd.DataFrame a GSLIBDataFrame is returned as a result of manipulation. . df = GSLIBDataFrame.read(&quot;data/example.dat&quot;, favorite_column=&quot;var&quot;) returned_df = df.applymap(lambda x: x*2) type(returned_df) . __main__.GSLIBDataFrame . Subclassing pd.DataFrame requires a bit more effort and has some quirks but in the long run might be worthwhile in cases where more than just a few methods and/or properites are going to be added to the class. If you&#39;re going down this road, have a look at the Pandas Documentation: Extending Pandas, much of what I shared here is paraphrased from their fantastic documentation. Code snippets posted as gists on github. .",
            "url": "geostats.dev/python/jupyter/pandas/gslib/oop/2020/09/17/extending-pandas.html",
            "relUrl": "/python/jupyter/pandas/gslib/oop/2020/09/17/extending-pandas.html",
            "date": " • Sep 17, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Importing and Exporting GSLIB (GEO-EAS) Files",
            "content": "Though a bit dated GSLIB remains the standard in many Geostatistical workflows, unfortunately the GSLIB data format can be a bit of hassle. The standard GSLIB aka GEO-EAS data format as described on gslib.com: . The first line in the file is taken as a title and is possibly transferred to output files. | The second line should be a numerical value specifying the number of numerical variables nvar in the data file. | The next nvar lines contain character identification labels and additional text (optional) that describe each variable. | The following lines, from nvar+3 until the end of the file, are considered as data points and must have nvar numerical values per line. Missing values are typically considered as large negative or positive numbers (e.g., less than -1.0e21 or greater than 1.0e21). The number of data will be the number of lines in the file minus nvar+2 minus the number of missing values. The programs read numerical values and not alphanumeric characters; alphanumeric variables may be transformed to integers or the source code modified. | . The header is informative, but not convienent for importing into Pandas. It should be noted that line #2 in the header can often contain grid definition information in addition to ncols, and in the case of multiple simulations nsim is commonly given after the grid definition (this is overlooked in the read/write functions to follow). . The goal here is just to provide a couple simple functions to save a little time for anyone who needs to do this. . Reading GSLIB data . Importing GSLIB data really happens in 2 steps. . 1. read the header 2. read all the data to a dataframe. . side note:I&#39;ve found skip_rows and delim_whitespace are useful when it comes to reading ASCII data from other scientific software (MODFLOW, PEST, TOUGH2 etc.) . def read_gslib(filename:str): with open(filename, &quot;r&quot;) as f: lines = f.readlines() ncols = int(lines[1].split()[0]) col_names = [lines[i+2].strip() for i in range(ncols)] df = pd.read_csv(filename, skiprows=ncols+2, delim_whitespace=True, names=col_names) return df . df = read_gslib(filename=&quot;data/example.dat&quot;) df.head() . x y z var . 0 0.723 | 0.564 | 0.785 | 2.853 | . 1 0.915 | 0.317 | 0.357 | 0.749 | . 2 0.346 | 0.484 | 0.690 | 0.786 | . 3 0.591 | 0.150 | 0.669 | 0.290 | . 4 0.157 | 0.332 | 0.006 | 1.777 | . Now go about your business analyzing data, making plots and doing all the other things python does well until you need re-export to GSLIB to run specific Geostatistical algorithm. . Writing a Pandas DataFrame to GSLIB Format . As with reading in the data, I&#39;m sure there are a number of ways this can be done. Below is one rather simple approach where I write the header than iterate over each row as a tuple. . If speed is a consideration when iterating over a pandas DataFrame use .itertuples its noticeably faster than .iterrows. . def write_gslib(df:pd.DataFrame, filename:str): with open(filename, &quot;w&quot;) as f: f.write(&quot;GSLIB Example Data n&quot;) f.write(f&quot;{len(df.columns)} n&quot;) f.write(&quot; n&quot;.join(df.columns)+&quot; n&quot;) for row in df.itertuples(): row_data = &quot; t&quot;.join([f&quot;{i:.3f}&quot; for i in row[1:]]) f.write(f&quot;{row_data} n&quot;) . write_gslib(df, &quot;data/exported_data.dat&quot;) . Now, just have a quick look at the file to be sure its correct: . with open(&quot;data/exported_data.dat&quot;,&quot;r&quot;) as f: for i in range(10): print(f.readline().strip()) . GSLIB Example Data 4 x y z var 0.723 0.564 0.785 2.853 0.915 0.317 0.357 0.749 0.346 0.484 0.690 0.786 0.591 0.150 0.669 0.290 . Really the whole purpose here is to have these functions readily available to copy/paste when you need them. .",
            "url": "geostats.dev/jupyter/gslib/pandas/geostatistics/python/2020/09/14/pandas-and-gslib.html",
            "relUrl": "/jupyter/gslib/pandas/geostatistics/python/2020/09/14/pandas-and-gslib.html",
            "date": " • Sep 14, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "First Post",
            "content": "Why? . In the last few years I’ve slowly started a list of things that were interesting, challenging or particularly useful that I’ve encountered working with python. The mental list turned into a written list and now a blog for a few reasons. . Its an opportunity to write down some of these tid-bits so I don’t forget them and have a reference next time something similar comes up. | Fastpages looked like a fun low barrier to entry blogging platform that makes it easy to share some code from a notebook, markdown or even a word file. | self-promotion. I’m generally terrible at this but every time I sign up for something new they ask for a github profile or personal website. If you’ve seen my github profile it needs some work… so here we are. | Does the world need another python blog? . Maybe, but probably not. . There are a lot great resources out there for learning python. Even so, I decided to start my own little page on the internet to share a few of the things I’ve learned and what I’m working on. I can’t promise code shared here will be absolutely perfect, but maybe it will be interesting and hopefully it will save you some time if you’re working on something similar. . Geoscience . My academic and professional background is in the geosciences (Geostatistics and Geology) and over the last 10 years I’ve found Python to be an amazing [nearly] do it all tool to automate and improve day to day tasks. My interests in geoscience focused on (but not limited to) subsurface modeling. In most cases this implies some knowledge and a limited amount of data from subsurface and building models to predict whats going in the areas without existing data. . Python . I’m not sure where I heard it, but I was once told “Python is the second best language for everything”, and I think thats true. In many cases there is another tool that might be better suited for a single specific task but as soon as you are connecting a few specific tasks into a larger workflow python outshines the other options as the best tool for the job. Python comes with drawbacks, out of the box it isn’t the fastest language out there but its fun to write, easy to read and has a rapidly growing set of tools for almost anything you can think of. . Goals . My goal is to add a post at least every few weeks. I don’t really want to get into the weeds on Geostatistics (but if thats of interest let me know!), I’d rather share some small ah ha moments and lessons learned the hard way using python. Hopefully these snippets will be useful to others outside of my own little niche in the geoscience world as well. .",
            "url": "geostats.dev/python/geoscience/about%20me/2020/09/13/first-post.html",
            "relUrl": "/python/geoscience/about%20me/2020/09/13/first-post.html",
            "date": " • Sep 13, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi, I’m Eric - Geostatistician &amp; Python Developer. . I started my career as Geologist in the in mining and exploration field. After a bit of numerical modeling and light introduction to Python I continued my studies at the University of Alberta’s Centre for Computational Geostatistics (CCG). My own research with Dr. Clayton Deutsch was focused on Prediction of Local Uncertainty and Localization. . My interest and experience in Python started with utilitarian scripts to string together a few FORTRAN programs. Since then I’ve gained an appreciation for Python and programming in a broader context - Machine Learning, Data Visualization, Reproducible/Shareable Workflows in Jupyter Notebooks and just a touch of Web Dev. .",
          "url": "geostats.dev/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
      ,"page3": {
          "title": "Resources",
          "content": "Learn Python . Real Python - Originally Michael Herman (TestDriven.io) now owned/run by Dan Bader, Real Python provides tutorials in both text, video and course format on all things python. I’ve learned a great deal from Real Python and highly recommend them. . | Talk Python To Me, Audio seems like a strange medium to learn anything code related but trust me this podcast is great. If you’ve got a background in something other than web development/computer science Talk Python To Me is a great way to get exposed to and learn from the wider world of python. Michael Kennedy does a great job. . | . Learn Geostatistics . GeostatsGuy aka Michael Pyrcz is a Professor at the University of Texas at Austin teaching Geostatistics and Machine Learning. Many (maybe all?) of his lectures are available on YouTube and they’re great for everyone beginner to expert. . | LazyModelingCrew This a group of somewhat recent graduates from the Centre for Computational Geostatistics at the University of Alberta. They’ve recently started sharing some easily digestible Geostatistics lessons as blog posts. I especially recommend the variogram focused posts for anyone new to geostats. . | Geostatistics Lessons Thoughtfully put together lessons for some of the more complex aspects of Geostatics. Mostly supported by the Centre for Computational Geostatics at the University of Alberta. . | . Tools, Packages etc for Geostatistics and Geoscience . AR2GAS . First a plug for the product I work on day to day: AR2GAS is c++ backed software with a Python API that can be run on your local machine, on a local cluster or on the cloud for geostatics on a regular or unstructured grid (or even gridless!). I think we’re doing great work and if you’re interested in learning more please reach out. . Open Source Geostatistics . There are a number of open source python packages available for Geostatistics using Python but my experience with them is limited and I can’t speak to how “production ready” they are. At some point I’d like to do a little compare/contrast/benchmark with a few of these. . SGeMS . SGeMS is a GUI based software for Geostatistics created at Stanford University. If you prefer clicking buttons to writing code this is a great option for free software. | . GSLIB . GSLIB is the original Geostatics Library of software created by Clayton Deutsch that has grown over the years. Its not fancy, but it works and you can always script a few programs together with subprocess. | . Python(ish) . The (ish) because a few of these use FORTRAN or C++ for the heavy lifting. . Pygeostat - Supported by Centre for Computational Geostatistics. Note much of the computation relies heavily on GSLIB FORTRAN programs. | GeostatsGuy - Michael Pyrcz’s python package for geostats. There are 2 components, one is pure python the other relies on GSLIB FORTRAN programs. | High Performance Geostatistics Library written in c++ with a python API - looks interesting and they give some benchmarks showing its substantially faster than GSLIB. | . I don’t know much about these other offerings but they have a number of stars on github so there are some people out there using them. . GSTOOls - Last commit 6 months ago.. | SciKit-Gstat - maybe still maintained? Last commit was 5 months ago. | .",
          "url": "geostats.dev/resources/",
          "relUrl": "/resources/",
          "date": ""
      }
      
  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "geostats.dev/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}