{
  
    
        "post0": {
            "title": "Packing a Dash App with PyFladesk and PyInstaller",
            "content": "Packing Dash Apps into an Executable . The first question is - why bother? This is a very valid question. Dash Apps are built for the web and best run in the browser, hosted on a server. But there are times when a stand alone app can be useful. . Mostly I wanted to see if it is feasible to create a small desktop GUI based tool (similar to Electron based JS software) with Dash. Turns out, its not hard - I’m sure there are some details to this that could be tuned up but the purpose of this post is to share the basic workflow for packing a dash app. . Step 1: Build A Dash App . Diamond Drillhole Quality-Assurance Quality-Control (DDH-QAQC) is a small dash app designed to desurvey and composite mining drill hole data as well as provide some basic data visualization for QA/QC purposes. This is a work in progress and hasn’t been optimized, as previously mentioned the main goal here was to try packing a dash app. . This App takes advantage of a couple nice Dash add-ons including dash_router, Dash Bootstrap Components and Dash DataTable. . The app itself is used to load some data from a few CSV files and do a bit of math to generate average values with x, y, coordinates. Ultimately, the user will want to compare the composited values to the original values and export the results once satisfied. To do all this, I added a very basic use of sqllite to support all this. . The app code can be seen on the DDH-QAQC Repo. A few screen shots in the next section.. . Step 2: Running Dash using QtWebEngine with PyFladesk . Under the hood Dash is built on flask, this makes implementing PyFladesk easy. Really its a single line, instead of the usual app.run_server use init_gui(app.server). There are a handful of other options to set the screen size, icon, port. Heres a snippet of the current setup: . if __name__ == &quot;__main__&quot;: from pyfladesk import init_gui init_gui( app.server, port=5000, width=1200, height=800, window_title=&quot;DDH-QAQC&quot;, icon=&quot;App/assets/favicon.ico&quot;, argv=None, ) . Ultimately, its super easy and works great: . . Step 3: Packaging it all with PyInstaller . There are a few tools available for packaging python into an executable. Used PyInstaller and I’ve heard cx_freeze works well too but its not something I’ve tried. Creating the executable took a bit of trial and error. For some reason the dash dependencies weren’t picked up automatically and had top be included in th main-dir.spec file manually: . import os import site site_pkgs_dir = site.getsitepackages()[1] addl_pkgs = [&quot;plotly&quot;, &quot;dash_renderer&quot;,&quot;dash_html_components&quot;,&quot;dash_bootstrap_components&quot;, &quot;dash_core_components&quot;, &quot;dash_table&quot;] a = Analysis([&#39;main.py&#39;], pathex=[&#39;c: code-dev ddh-qaqc App&#39;], binaries=[], datas=[(os.path.join(site_pkgs_dir,pkg),pkg) for pkg in addl_pkgs], . Also, I ran into some trouble with flask-compress. See this Stack Overflow question for the solution. This was copied to the app.py file prior to importing dash. . With all that taken care of, creating an executable that includes a directory full of dependencies works nicely. There is a flag to run PyInstaller and produce a single file but for some reason this simply wasn’t working. For now, the directory based solution works fine and is available here should you want to give it a try. . Conclusion . There are plenty of great ways to create desktop based software, but this approach makes it pretty damn easy - even if its not perfect. If you have a need to make nice looking GUI and don’t want to get building a full on React/Electron app Dash with PyFladesk and PyInstaller is an option. Give it a go, would love to see more creative uses of Dash! .",
            "url": "geostats.dev/python/plotly/dash/flask/dash%20bootstrap%20components/pyfladesk/pyinstaller/2021/03/14/dash-packing.html",
            "relUrl": "/python/plotly/dash/flask/dash%20bootstrap%20components/pyfladesk/pyinstaller/2021/03/14/dash-packing.html",
            "date": " • Mar 14, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Better Dashboards with Dash",
            "content": "Tips for Creating Dashboards with Dash . Presenting data visualizations via a dashboard is a great way to communicate insights from data and provide some tools for decision making. There are many many different tools both commercial and open source available for dashboarding, one of my favorites in the open source category is Dash. Dash is free (though there is an enterprise version) and makes it possible to host a series of plotly charts, tables, visualizations on a flask based webapp [almost] exclusively using python. . Start With Better Plots . The first few items on this list aren’t specifically related to dash, but plotly. Better looking plots yield a better looking dashboard. The details matter, and like most plotting packages the defaults rarely fit your needs. The following are a few items from the plotly docs worth noting. . Note: for all the plotly examples assume import plotly.graph_objs as go . Themes &amp; Templates . If the default colors and theme from plotly don’t fit your your desired color palette there are a handful of pre-configured themes. Most of the time these templates are a good starting point and a few adjustments to axes, colors etc will suffice. If thats not enough a custom template can be created and saved a json file and set to the default. The plotly docs give a good walk through of this feature. . Efficiently Instantiate Figures Using Magic Underscore Notation . The layout controls most aesthetic plot details such as labels, axes ranges, etc. In many cases this leads to a heavily nested statement to instantiate the go.Figure. The magic underscore notation makes accessing all of these details easier and more efficient. Here’s an example, with and without the magic underscore approach used to set the template and axes labels. . Without Underscore Notation: . f = go.figure(layout=dict(template=&quot;none&quot;, xaxis=dict(title_text=&quot;x values&quot;))) . With Underscore Notation: . f = go.figure(layout_template=&quot;none&quot;, layout_xaxis_title_text=&quot;x_values&quot;) . Aspect Ratio . Plotly does not enforce an equal aspect ratio by default. There are times when this is important, rather than having to dig through the docs heres the syntax make it happen: . In 2-D Plots, set the scaleanchor and scaleratio . f = go.Figure(layout_yaxis=dict(scaleanchor=&quot;x&quot;, scaleratio=1)) . In 3-D the aspect ratio is controlled by the scene aspectmode. . f = go.Figure(layout_scene_aspectmode=&quot;data&quot;) . Dash . The second section of this post focuses on Dash. Creating a single page Dash app is quick, easy and the docs provide great examples. However, to go beyond a single page app with static data you’ll have to get a bit more creative. The following are a few of my go-to tools and strategies for building dash apps. . Have a look at the dash-demo repo on my github to see a very basic working example. . Dash Bootstrap Components . I find web development a bit intimidating. Not only are there a lot of moving pieces but there are many ways to construct each of those pieces. Twitter bootstrap is just once css framework, but its well documented and has everything you need to build a good looking website. Dash Bootstrap Components (dbc) brings the layout, styling and a handful of components from bootstrap to dash with minimal code. Sure, you can include the css yourself but this is just easy. Also there are a few included themes from bootswatch which is an easy way to change the look of your app with a single line of code. . If you’re unfamiliar with boostrap, the layout section of the dbc docs gives all the overview you’ll need. Adding dbc to your project opens the ability to use any existing bootstrap classes as well. Have a look at the bootstrap docs for more info. . Routing . Running multiple dash apps or even a single app with multiple pages or dynamic layouts can be a bit of hassle. In my opinion the solution provided by plotly is insufficient. If the app grows to more than 2 or 3 urls a rather large ugly conditional statement becomes the central switchboard dispatching the layout based on url. Mercy University Hospital has come up with a much nicer solution called dash_router. This provides a flask-style decorator to handle the url routing. No, it doesn’t have ALL the features of flask, but its a handy way to manage a few different endpoints and pass a couple parameters in a dynamic url. I added a few lines of code to handle query strings as well (hence the link to my fork of the repo). . In the dash-demo example, this is added as separate module, router.py. The snippet below gives two examples, first a dynamic url that includes an argument passed on the to layout function and the second uses a query string for some n parameters passed on to the layout (handled by **kwargs). In this case these layouts plot either a single distribution selected from the drop down or multiple. . @router.route(&quot;/univariate/&lt;distribution_name&gt;&quot;) def univariate_stats(distribution_name): return template_layout(dcc.Graph(figure=plots.histogram(data[distribution_name]))) @router.route(&quot;/multivariate&quot;) def multivariate_stats(**kwargs): data_to_plot = [data[v] for k, v in kwargs.items()] return template_layout(dcc.Graph(figure=plots.histogram(*data_to_plot))) . Thanks to Mercy University Hospital for this, I’ve found this bit of code incredibly helpful in managing some larger dash apps with many pages. One item I have yet to try with this is sending a JSON payload via post to some url. Thats next on my to-do list. . Responsive Layout . One of the things I like best about Dash is being able to easily transfer plotly charts I’ve created in Jupyter Notebook/Lab and put them on a dashboard by dcc.Graph. In general this is painless and works well, but it can feel clunky without a few adjustments to make the layout responsive. . There are a few ways to do this but here is my preference: . Use Dash Bootstrap Component and be sure your layout is encapsulated by a dbc.Container with fluid=True. | Be sure any dcc.Graph component is in a dbc.Col. By doing this the default behavior is for the plot to automatically resize in the horizontal direction based on the viewport (aka browser window). | For plots to resize vertically, use viewport units to define the height. For a plot that takes up 50% of the viewport screen: figure=go.Figure(layout_height=&#39;50vh&#39;). (This can also be used for width, but I prefer to let the dbc.Col handle that by setting the width argument there.) | Note: If your plot has layout_height and/or layout_width defined by pixels (passing an integer to these keywords assumes pixels) the plot will not resize correctly. . It should also be mentioned that if you do not desire responsive plots responsive=False can be set in the dcc.Graph component. . Remove the Plotly Logo . Another quick note on plots. If you’d like to strip the plotly logo off the tool bar of your plots set displaylogo=False. Heres a histogram plotting example from the dash-demo repo: . dcc.Graph(figure=plots.histogram(data[distribution_name]), style={&quot;height&quot;: &quot;80vh&quot;}, config={ &quot;displaylogo&quot;: False, }, ) . Callback Triggers . Callbacks become central to any interactive dashboard made with Dash. A single callback yields a single html object, it is not possible to have multiple callbacks that control the same object. To get around this, it can be helpful to have a single callback, with multiple inputs and track which input is triggered. Based on the input triggered, different logic or actions can be applied to the resulting object. . Here is the key: trigger = dash.callback_context.triggered. Inside of your call back function this trigger can tell you which input was changed and its new value. . @app.callback( Output(&quot;my-output-div&quot;,&quot;children&quot;), [Input(&quot;input_a&quot;, &quot;value&quot;), Input(&quot;input_b&quot;,&quot;value&quot;)] ) def multiple_input_actions(value_a, value_b): trigger = dash.callback_context.triggered if trigger[0][&quot;propid&quot;] == &quot;input_a&quot;: # do something particular for input_a here return &quot;input_a triggered callback&quot; else: # do something else here for input_b return &quot;input_b triggered callback&quot; . Conclusion . Dash is great. There are a handful of alternatives (streamlit, panel, bokeh etc) they all have their place too, but I highly recommend dash if you need a professional looking dashboard but want to stop short of building a complete flask app. The above tips, snippets and strategies are things I use regularly and hopefully they’ll be helpful for others. If so, please let me know. Or, if you’ve got a better approach I’d love to hear that too. For a more complete example take a look at the dash-demo repo which includes examples of most of the items mentioned above. .",
            "url": "geostats.dev/python/plotly/dash/flask/dash%20bootstrap%20components/2020/11/26/dash-post.html",
            "relUrl": "/python/plotly/dash/flask/dash%20bootstrap%20components/2020/11/26/dash-post.html",
            "date": " • Nov 26, 2020"
        }
        
    
  
    
  
    
        ,"post3": {
            "title": "Getting Started with Python on Windows",
            "content": "Getting Started with Python on Windows . Reading python how-to guides, blogs and even tweets often gives the illusion that all things python take place entirely on Linux and Mac OS. I’m on windows 99% of the time when using python and happy to report it works great. There is a bit of a lingering stigma regarding python on windows because there was a time when getting python installed and setup on a windows machine was a quite a hassle. Those days are over. In fact, there are more windows users than mac users according to the JetBrains survey of 24k python users: . . Good news, that is absolutely no longer the case. However, I do think getting started with Python on windows can be a bit confusing. The goal of this post is to give a brief, but opinionated summary of how to go from never having touched python to running code in a jupyter notebook ASAP. . Where to get Python . Google something along the lines of “install python windows” and you’ll get python.org, closely followed by realpython.com and then a Microsoft page suggesting a python download from the Microsoft Appstore. Instead of these options, seek out Anaconda. This is a cross-platform solution to getting python installed and running with ease. . The Anaconda distribution includes: . The latest version of python (create an environment for other versions needed - more on that to come) | conda command line tool | A substantial set of pre-installed packages (particularly useful for scientific computing or data science) | Anaconda Navigator (for those that prefer a GUI to launch python based tools) | . Note: that if you desire a lighter weight version of Anaconda without the pre-installed packages consider miniconda . Starting Jupyter . Jupyter Notebooks and Jupyter Lab are an approachable way to get started writing some code for beginners, as well as a fantastic way to combine data visualization with text and blocks of code for more experienced users. . To get started either launch anaconda navigator from the start menu, or open a command line terminal, navigate to your working directory and open by either jupyter notebookor jupyter lab. . For some the command line terminal is unfamiliar or intimidating - instead jupyter notebook or lab can be launched from the file explorer with the same command (juptyer notebook or jupter lab) entered in the address bar: . . Virtual Environments with . A virtual environment can be thought of as an isolated install of python with a set of packages separate from the system (or user) -wide install. A virtual environment provides the freedom to work on projects that require a different version of python, or necessitate package(s) of different versions despite the latest and greatest that was installed with the Anaconda download. Additionally, a virtual environment can be replicated in another location or on another machine ensuring the ability to run your code wherever its needed. Shared below are some common conda commands for creating/using/cloning environments. If you need more info the conda documentation is excellent. . Environment Setup . Creating a new environment is simple . conda env create -n mynewenv (replacing mynewenv with a more appropriate name). . Anytime you want to use this environment, activate it . conda activate mynewenv . Once activated, anytime you call python or it will call the version specific to that environment, isolated from the global install. . Note: Using Jupyter (Notebook or Lab) within a Conda Environment requires an extra step. . If you want to use your conda environment in a jupyter notebook : . conda install ipykernel nb_conda_kernels . ipykernel provides everything you need to run jupyter from your new environment | nb_conda_kernels provides the added bonus of being able to access other conda environments on your system from within jupyter | . Copying an Environment . Ultimately, you’ll want to share your code with a friend or colleague to replicate some python workflow on a new machine or new data elsewhere. Using a Virtual Environment makes this easy. To replicate an environment, simply record all packages being used and their version number. To clone an environment on the same machine: . conda create --name myenvclone --clone myoldenv . Conda also provides a convenient way to export all the pertinent information to a YAML file. From within the environment you’d like to copy: . conda env export &gt; environment.yml . Now, take this environment.yml and recreate that exact same environment on a different machine: . conda env create -f environment.yml . Note: The YAML approach for saving environment information does not replace the standard requirements.txt that is common with pip. Instead, the conda approach is best suited for exactly the process described above: cloning an environment. Same name, all the same packages etc. requirements.txt is best suited for a list dependencies required for some package, stopping short of creating an entire environment. . Conclusion . Python works great on windows, don’t let anyone tell you otherwise. If you’re working on windows, especially in the realm of scientific computing or data science, Anaconda is highly recommended. Once you’ve got Anaconda set up and are comfortable with the basics, consider working a virtual environment for most things you’re doing. Does absolutely every bit of code need its own environment, definitely not but for any substantial project its highly recommended. .",
            "url": "geostats.dev/beginner/python/anaconda/conda/virtual%20environments/2020/10/08/python-on-windows.html",
            "relUrl": "/beginner/python/anaconda/conda/virtual%20environments/2020/10/08/python-on-windows.html",
            "date": " • Oct 8, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Pandas DataFrames with a GSLIB I/O Methods",
            "content": "Pandas is everywhere, for good reason. If you are using python to manipulate data, chances are you&#39;re using pandas. If you are using python for geoscience, you&#39;ve likely come across some weird FORTRAN generated file format that was a great idea in the 80s, but is a bit of a hassle for I/O operations. Pandas offers a large number of read/write methods, but occasionally some archaic file format comes along that can be a challenge (MODFLOW anyone?). But don&#39;t limit yourself to I/O operations, if there is additional functionality you desire from Pandas, you can extend dataframe, series or index functionality to do just that. If extending existing Pandas objects isn&#39;t enough, with a little extra effort and a few important details subclass and make your own DataFrame class. . Step 1: Reusable Function . In the last post about reading/writing GSLIB files, I shared a couple short snippets I use for reading/writing GEO-EAS (GSLIB) files to/from Pandas. As an example, I&#39;ll extend pandas to include these convenient read/write operations: . def write_gslib(self, filename:str): with open(filename, &quot;w&quot;) as f: f.write(&quot;GSLIB Example Data n&quot;) f.write(f&quot;{len(self._obj.columns)} n&quot;) f.write(&quot; n&quot;.join(self._obj.columns)+&quot; n&quot;) for row in df.itertuples(): row_data = &quot; t&quot;.join([f&quot;{i:.3f}&quot; for i in row[1:]]) f.write(f&quot;{row_data} n&quot;) def read_gslib(filename:str): with open(filename, &quot;r&quot;) as f: lines = f.readlines() ncols = int(lines[1].split()[0]) col_names = [lines[i+2].strip() for i in range(ncols)] df = pd.read_csv(filename, skiprows=ncols+2, delim_whitespace=True, names=col_names) return df . Step 2: pd.DataFrame Accessor . I&#39;m a big fan of this Pandas functionality. Just a simple decorator opens up the ability to add your own methods, properties etc. Here are the steps: . Make a class out your function(s). I&#39;ll call mine . GSLIBAccessor: . | class GSLIBAccessor: def __init__(self, pandas_obj): self._obj=(pandas_obj) def write_gslib(self, filename:str): with open(filename, &quot;w&quot;) as f: f.write(&quot;GSLIB Example Data n&quot;) f.write(f&quot;{len(self._obj.columns)} n&quot;) f.write(&quot; n&quot;.join(self._obj.columns)+&quot; n&quot;) for row in df.itertuples(): row_data = &quot; t&quot;.join([f&quot;{i:.3f}&quot; for i in row[1:]]) f.write(f&quot;{row_data} n&quot;) def read_gslib(filename:str): with open(filename, &quot;r&quot;) as f: lines = f.readlines() ncols = int(lines[1].split()[0]) col_names = [lines[i+2].strip() for i in range(ncols)] df = pd.read_csv(filename, skiprows=ncols+2, delim_whitespace=True, names=col_names) return df . In the __init__ include your DataFrame that this method will be operating on, pandas_obj in above snippet. | Add the decorator (Also I removed the _gslib suffix): | @pd.api.extensions.register_dataframe_accessor(&quot;gslib&quot;) class GSLIBAccessor: def __init__(self, pandas_obj): self._obj=(pandas_obj) def write(self, filename:str): with open(filename, &quot;w&quot;) as f: f.write(&quot;GSLIB Example Data n&quot;) f.write(f&quot;{len(self._obj.columns)} n&quot;) f.write(&quot; n&quot;.join(self._obj.columns)+&quot; n&quot;) for row in df.itertuples(): row_data = &quot; t&quot;.join([f&quot;{i:.3f}&quot; for i in row[1:]]) f.write(f&quot;{row_data} n&quot;) @staticmethod def read(filename:str): with open(filename, &quot;r&quot;) as f: lines = f.readlines() ncols = int(lines[1].split()[0]) col_names = [lines[i+2].strip() for i in range(ncols)] df = pd.read_csv(filename, skiprows=ncols+2, delim_whitespace=True, names=col_names) return df . Boom! Done! Thats it. Fantastic, right? . Now, the functionwrite_gslib is available as a DataFrame method at df.gslib.write() . There are plenty of GSLIB specific details to manage here - fill null values with -999.00, add grid definition in the file header etc, but the point here is this is a fast, easy, flexible way to add whatever functionality you need to the DataFrame class. . Reading in a dataframe by this approach works just fine as well. Keep in mind, the method created is associated with the DataFrame and won&#39;t be accessible at same place as the other pandas I/O operations like pd.read_csv, instead it will be at pd.DataFrame.gslib.read. . df = pd.DataFrame.gslib.read(&quot;data/example.dat&quot;) df.head() . x y z var . 0 0.723 | 0.564 | 0.785 | 2.853 | . 1 0.915 | 0.317 | 0.357 | 0.749 | . 2 0.346 | 0.484 | 0.690 | 0.786 | . 3 0.591 | 0.150 | 0.669 | 0.290 | . 4 0.157 | 0.332 | 0.006 | 1.777 | . To write out the file: . df.gslib.write(&quot;data/export_data.dat&quot;) . with open(&quot;data/export_data.dat&quot;, &quot;r&quot;) as f: for i in range(10): print(f.readline().strip()) . GSLIB Example Data 4 x y z var 0.723 0.564 0.785 2.853 0.915 0.317 0.357 0.749 0.346 0.484 0.690 0.786 0.591 0.150 0.669 0.290 . Step 3: Subclassing Pandas DataFrame . If forwhatever reason the decorator accessor approach isn&#39;t enough, you can always create your own class entirely and inheirit pd.DataFrame. This is a bit more work and there are a couple import details not to be missed. . Inheiritance . Inheiritance is a common aspect of OOP (object oriented programming), and is a topic that warrants a discussion all its own. Rather than get into that can of worms, if want some more details I&#39;d suggest Real Python: Inheiritance and Composition. In example, create a new class and inheirit all the good things that pd.DataFrame does, but add a few properities, methods etc. . class GSLIBDataFrame(pd.DataFrame): def __init__(self, data, *args, **kwargs): super().__init__(data=data, *args, **kwargs) def write(self, filename:str): with open(filename, &quot;w&quot;) as f: f.write(&quot;GSLIB Example Data n&quot;) f.write(f&quot;{len(self._obj.columns)} n&quot;) f.write(&quot; n&quot;.join(self._obj.columns)+&quot; n&quot;) for row in df.itertuples(): row_data = &quot; t&quot;.join([f&quot;{i:.3f}&quot; for i in row[1:]]) f.write(f&quot;{row_data} n&quot;) @classmethod def read(cls, filename:str): with open(filename, &quot;r&quot;) as f: lines = f.readlines() ncols = int(lines[1].split()[0]) col_names = [lines[i+2].strip() for i in range(ncols)] df = pd.read_csv(filename, skiprows=ncols+2, delim_whitespace=True, names=col_names) return cls(df) . This works and is quite similar to the accessor example but I do think this apporach would scale better should you have big plans for your DIY dataframes. Lets consider a couple issues shown below . df = GSLIBDataFrame.read(&quot;data/example.dat&quot;) returned_df = df.applymap(lambda x: x*2) type(returned_df) . pandas.core.frame.DataFrame . If you use some standard pandas operatione, it will return a regular pd.DataFrame not a GSLIBDataFrame - To manage this, the _constructor must be defined to override the method inheirted from pandas. . | Any other properites created, but be added to the metadata list so that they are passed on to results of manipulation. . | To demonstrate in addition to the previously defined methods, add a property favorite_column, though this name is nonsense this can be a useful approach for defining a specific column that defines categories, domains or coordinates. . class GSLIBDataFrame(pd.DataFrame): def __init__(self, data, favorite_column=None, *args, **kwargs): super().__init__(data=data, *args, **kwargs) self.favorite_column = favorite_column _metadata = [&quot;favorite_column&quot;] @property def _constructor(self): return GSLIBDataFrame def write(self, filename:str): with open(filename, &quot;w&quot;) as f: f.write(&quot;GSLIB Example Data n&quot;) f.write(f&quot;{len(self._obj.columns)} n&quot;) f.write(&quot; n&quot;.join(self._obj.columns)+&quot; n&quot;) for row in df.itertuples(): row_data = &quot; t&quot;.join([f&quot;{i:.3f}&quot; for i in row[1:]]) f.write(f&quot;{row_data} n&quot;) @classmethod def read(cls, filename:str, favorite_column:str): with open(filename, &quot;r&quot;) as f: lines = f.readlines() ncols = int(lines[1].split()[0]) col_names = [lines[i+2].strip() for i in range(ncols)] df = pd.read_csv(filename, skiprows=ncols+2, delim_whitespace=True, names=col_names) return cls(data=df, favorite_column=favorite_column) . Now, instead of returning a pd.DataFrame a GSLIBDataFrame is returned as a result of manipulation. . df = GSLIBDataFrame.read(&quot;data/example.dat&quot;, favorite_column=&quot;var&quot;) returned_df = df.applymap(lambda x: x*2) type(returned_df) . __main__.GSLIBDataFrame . Subclassing pd.DataFrame requires a bit more effort and has some quirks but in the long run might be worthwhile in cases where more than just a few methods and/or properites are going to be added to the class. If you&#39;re going down this road, have a look at the Pandas Documentation: Extending Pandas, much of what I shared here is paraphrased from their fantastic documentation. Code snippets posted as gists on github. .",
            "url": "geostats.dev/python/jupyter/pandas/gslib/oop/2020/09/17/extending-pandas.html",
            "relUrl": "/python/jupyter/pandas/gslib/oop/2020/09/17/extending-pandas.html",
            "date": " • Sep 17, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Importing and Exporting GSLIB (GEO-EAS) Files",
            "content": "Though a bit dated GSLIB remains the standard in many Geostatistical workflows, unfortunately the GSLIB data format can be a bit of hassle. The standard GSLIB aka GEO-EAS data format as described on gslib.com: . The first line in the file is taken as a title and is possibly transferred to output files. | The second line should be a numerical value specifying the number of numerical variables nvar in the data file. | The next nvar lines contain character identification labels and additional text (optional) that describe each variable. | The following lines, from nvar+3 until the end of the file, are considered as data points and must have nvar numerical values per line. Missing values are typically considered as large negative or positive numbers (e.g., less than -1.0e21 or greater than 1.0e21). The number of data will be the number of lines in the file minus nvar+2 minus the number of missing values. The programs read numerical values and not alphanumeric characters; alphanumeric variables may be transformed to integers or the source code modified. | . The header is informative, but not convienent for importing into Pandas. It should be noted that line #2 in the header can often contain grid definition information in addition to ncols, and in the case of multiple simulations nsim is commonly given after the grid definition (this is overlooked in the read/write functions to follow). . The goal here is just to provide a couple simple functions to save a little time for anyone who needs to do this. . Reading GSLIB data . Importing GSLIB data really happens in 2 steps. . 1. read the header 2. read all the data to a dataframe. . side note:I&#39;ve found skip_rows and delim_whitespace are useful when it comes to reading ASCII data from other scientific software (MODFLOW, PEST, TOUGH2 etc.) . def read_gslib(filename:str): with open(filename, &quot;r&quot;) as f: lines = f.readlines() ncols = int(lines[1].split()[0]) col_names = [lines[i+2].strip() for i in range(ncols)] df = pd.read_csv(filename, skiprows=ncols+2, delim_whitespace=True, names=col_names) return df . df = read_gslib(filename=&quot;data/example.dat&quot;) df.head() . x y z var . 0 0.723 | 0.564 | 0.785 | 2.853 | . 1 0.915 | 0.317 | 0.357 | 0.749 | . 2 0.346 | 0.484 | 0.690 | 0.786 | . 3 0.591 | 0.150 | 0.669 | 0.290 | . 4 0.157 | 0.332 | 0.006 | 1.777 | . Now go about your business analyzing data, making plots and doing all the other things python does well until you need re-export to GSLIB to run specific Geostatistical algorithm. . Writing a Pandas DataFrame to GSLIB Format . As with reading in the data, I&#39;m sure there are a number of ways this can be done. Below is one rather simple approach where I write the header than iterate over each row as a tuple. . If speed is a consideration when iterating over a pandas DataFrame use .itertuples its noticeably faster than .iterrows. . def write_gslib(df:pd.DataFrame, filename:str): with open(filename, &quot;w&quot;) as f: f.write(&quot;GSLIB Example Data n&quot;) f.write(f&quot;{len(df.columns)} n&quot;) f.write(&quot; n&quot;.join(df.columns)+&quot; n&quot;) for row in df.itertuples(): row_data = &quot; t&quot;.join([f&quot;{i:.3f}&quot; for i in row[1:]]) f.write(f&quot;{row_data} n&quot;) . write_gslib(df, &quot;data/exported_data.dat&quot;) . Now, just have a quick look at the file to be sure its correct: . with open(&quot;data/exported_data.dat&quot;,&quot;r&quot;) as f: for i in range(10): print(f.readline().strip()) . GSLIB Example Data 4 x y z var 0.723 0.564 0.785 2.853 0.915 0.317 0.357 0.749 0.346 0.484 0.690 0.786 0.591 0.150 0.669 0.290 . Really the whole purpose here is to have these functions readily available to copy/paste when you need them. .",
            "url": "geostats.dev/jupyter/gslib/pandas/geostatistics/python/2020/09/14/pandas-and-gslib.html",
            "relUrl": "/jupyter/gslib/pandas/geostatistics/python/2020/09/14/pandas-and-gslib.html",
            "date": " • Sep 14, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "First Post",
            "content": "Why? . In the last few years I’ve slowly started a list of things that were interesting, challenging or particularly useful that I’ve encountered working with python. The mental list turned into a written list and now a blog for a few reasons. . Its an opportunity to write down some of these tid-bits so I don’t forget them and have a reference next time something similar comes up. | Fastpages looked like a fun low barrier to entry blogging platform that makes it easy to share some code from a notebook, markdown or even a word file. | self-promotion. I’m generally terrible at this but every time I sign up for something new they ask for a github profile or personal website. If you’ve seen my github profile it needs some work… so here we are. | Does the world need another python blog? . Maybe, but probably not. . There are a lot great resources out there for learning python. Even so, I decided to start my own little page on the internet to share a few of the things I’ve learned and what I’m working on. I can’t promise code shared here will be absolutely perfect, but maybe it will be interesting and hopefully it will save you some time if you’re working on something similar. . Geoscience . My academic and professional background is in the geosciences (Geostatistics and Geology) and over the last 10 years I’ve found Python to be an amazing [nearly] do it all tool to automate and improve day to day tasks. My interests in geoscience focused on (but not limited to) subsurface modeling. In most cases this implies some knowledge and a limited amount of data from subsurface and building models to predict whats going in the areas without existing data. . Python . I’m not sure where I heard it, but I was once told “Python is the second best language for everything”, and I think thats true. In many cases there is another tool that might be better suited for a single specific task but as soon as you are connecting a few specific tasks into a larger workflow python outshines the other options as the best tool for the job. Python comes with drawbacks, out of the box it isn’t the fastest language out there but its fun to write, easy to read and has a rapidly growing set of tools for almost anything you can think of. . Goals . My goal is to add a post at least every few weeks. I don’t really want to get into the weeds on Geostatistics (but if thats of interest let me know!), I’d rather share some small ah ha moments and lessons learned the hard way using python. Hopefully these snippets will be useful to others outside of my own little niche in the geoscience world as well. .",
            "url": "geostats.dev/python/geoscience/about%20me/2020/09/13/first-post.html",
            "relUrl": "/python/geoscience/about%20me/2020/09/13/first-post.html",
            "date": " • Sep 13, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi, I’m Eric - Geostatistician &amp; Python Developer. . I started my career as Geologist in the in mining and exploration field. After a bit of numerical modeling and light introduction to Python I continued my studies at the University of Alberta’s Centre for Computational Geostatistics (CCG). My own research with Dr. Clayton Deutsch was focused on Prediction of Local Uncertainty and Localization. . My interest and experience in Python started with utilitarian scripts to string together a few FORTRAN programs. Since then I’ve gained an appreciation for Python and programming in a broader context - Machine Learning, Data Visualization, Reproducible/Shareable Workflows in Jupyter Notebooks and just a touch of Web Dev. .",
          "url": "geostats.dev/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
      ,"page3": {
          "title": "Resources",
          "content": "Learn Python . Real Python - Originally Michael Herman (TestDriven.io) now owned/run by Dan Bader, Real Python provides tutorials in both text, video and course format on all things python. I’ve learned a great deal from Real Python and highly recommend them. . | Talk Python To Me, Audio seems like a strange medium to learn anything code related but trust me this podcast is great. If you’ve got a background in something other than web development/computer science Talk Python To Me is a great way to get exposed to and learn from the wider world of python. Michael Kennedy does a great job. . | . Learn Geostatistics . GeostatsGuy aka Michael Pyrcz is a Professor at the University of Texas at Austin teaching Geostatistics and Machine Learning. Many (maybe all?) of his lectures are available on YouTube and they’re great for everyone beginner to expert. . | LazyModelingCrew This a group of somewhat recent graduates from the Centre for Computational Geostatistics at the University of Alberta. They’ve recently started sharing some easily digestible Geostatistics lessons as blog posts. I especially recommend the variogram focused posts for anyone new to geostats. . | Geostatistics Lessons Thoughtfully put together lessons for some of the more complex aspects of Geostatics. Mostly supported by the Centre for Computational Geostatics at the University of Alberta. . | . Tools, Packages etc for Geostatistics and Geoscience . AR2GAS . First a plug for the product I work on day to day: AR2GAS is c++ backed software with a Python API that can be run on your local machine, on a local cluster or on the cloud for geostatics on a regular or unstructured grid (or even gridless!). I think we’re doing great work and if you’re interested in learning more please reach out. . Open Source Geostatistics . There are a number of open source python packages available for Geostatistics using Python but my experience with them is limited and I can’t speak to how “production ready” they are. At some point I’d like to do a little compare/contrast/benchmark with a few of these. . SGeMS . SGeMS is a GUI based software for Geostatistics created at Stanford University. If you prefer clicking buttons to writing code this is a great option for free software. | . GSLIB . GSLIB is the original Geostatics Library of software created by Clayton Deutsch that has grown over the years. Its not fancy, but it works and you can always script a few programs together with subprocess. | . Python(ish) . The (ish) because a few of these use FORTRAN or C++ for the heavy lifting. . Pygeostat - Supported by Centre for Computational Geostatistics. Note much of the computation relies heavily on GSLIB FORTRAN programs. | GeostatsGuy - Michael Pyrcz’s python package for geostats. There are 2 components, one is pure python the other relies on GSLIB FORTRAN programs. | High Performance Geostatistics Library written in c++ with a python API - looks interesting and they give some benchmarks showing its substantially faster than GSLIB. | . I don’t know much about these other offerings but they have a number of stars on github so there are some people out there using them. . GSTOOls - Last commit 6 months ago.. | SciKit-Gstat - maybe still maintained? Last commit was 5 months ago. | .",
          "url": "geostats.dev/resources/",
          "relUrl": "/resources/",
          "date": ""
      }
      
  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "geostats.dev/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}