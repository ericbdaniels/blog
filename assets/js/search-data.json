{
  
    
        "post0": {
            "title": "Better Dashboards with Dash",
            "content": "Tips for Creating Dashboards with Dash . Presenting data visualizations via a dashboard is a great way to communicate insights from data and provide some tools for decision making. There are many many different tools both commercial and open source available for dashboarding, one of my favorites in the open source category is Dash. Dash is free (though there is an enterprise version) and makes it possible to host a series of plotly charts, tables, visualizations on a flask based webapp [almost] exclusively using python. . Start With Better Plots . The first few items on this list aren’t specifically related to dash, but plotly. Better looking plots yield a better looking dashboard. The details matter, and like most plotting packages the defaults rarely fit your needs. The following are a few items from the plotly docs worth noting. . Note: for all the plotly examples assume import plotly.graph_objs as go . Themes &amp; Templates . If the default colors and theme from plotly don’t fit your your desired color palette there are a handful of pre-configured themes. Most of the time these templates are a good starting point and a few adjustments to axes, colors etc will suffice. If thats not enough a custom template can be created and saved a json file and set to the default. The plotly docs give a good walk through of this feature. . Efficiently Instantiate Figures Using Magic Underscore Notation . The layout controls most aesthetic plot details such as labels, axes ranges, etc. In many cases this leads to a heavily nested statement to instantiate the go.Figure. The magic underscore notation makes accessing all of these details easier and more efficient. Here’s an example, with and without the magic underscore approach used to set the template and axes labels. . Without Underscore Notation: . f = go.figure(layout=dict(template=&quot;none&quot;, xaxis=dict(title_text=&quot;x values&quot;))) . With Underscore Notation: . f = go.figure(layout_template=&quot;none&quot;, layout_xaxis_title_text=&quot;x_values&quot;) . Aspect Ratio . Plotly does not enforce an equal aspect ratio by default. There are times when this is important, rather than having to dig through the docs heres the syntax make it happen: . In 2-D Plots, set the scaleanchor and scaleratio . f = go.Figure(layout_yaxis=dict(scaleanchor=&quot;x&quot;, scaleratio=1)) . In 3-D the aspect ratio is controlled by the scene aspectmode. . f = go.Figure(layout_scene_aspectmode=&quot;data&quot;) . Dash . The second section of this post focuses on Dash. Creating a single page Dash app is quick, easy and the docs provide great examples. However, to go beyond a single page app with static data you’ll have to get a bit more creative. The following are a few of my go-to tools and strategies for building dash apps. . Have a look at the dash-demo repo on my github to see a very basic working example. . Dash Bootstrap Components . I find web development a bit intimidating. Not only are there a lot of moving pieces but there are many ways to construct each of those pieces. Twitter bootstrap is just once css framework, but its well documented and has everything you need to build a good looking website. Dash Bootstrap Components (dbc) brings the layout, styling and a handful of components from bootstrap to dash with minimal code. Sure, you can include the css yourself but this is just easy. Also there are a few included themes from bootswatch which is an easy way to change the look of your app with a single line of code. . If you’re unfamiliar with boostrap, the layout section of the dbc docs gives all the overview you’ll need. Adding dbc to your project opens the ability to use any existing bootstrap classes as well. Have a look at the bootstrap docs for more info. . Routing . Running multiple dash apps or even a single app with multiple pages or dynamic layouts can be a bit of hassle. In my opinion the solution provided by plotly is insufficient. If the app grows to more than 2 or 3 urls a rather large ugly conditional statement becomes the central switchboard dispatching the layout based on url. Mercy University Hospital has come up with a much nicer solution called dash_router. This provides a flask-style decorator to handle the url routing. No, it doesn’t have ALL the features of flask, but its a handy way to manage a few different endpoints and pass a couple parameters in a dynamic url. I added a few lines of code to handle query strings as well (hence the link to my fork of the repo). . In the dash-demo example, this is added as separate module, router.py. The snippet below gives two examples, first a dynamic url that includes an argument passed on the to layout function and the second uses a query string for some n parameters passed on to the layout (handled by **kwargs). In this case these layouts plot either a single distribution selected from the drop down or multiple. . @router.route(&quot;/univariate/&lt;distribution_name&gt;&quot;) def univariate_stats(distribution_name): return template_layout(dcc.Graph(figure=plots.histogram(data[distribution_name]))) @router.route(&quot;/multivariate&quot;) def multivariate_stats(**kwargs): data_to_plot = [data[v] for k, v in kwargs.items()] return template_layout(dcc.Graph(figure=plots.histogram(*data_to_plot))) . Thanks to Mercy University Hospital for this, I’ve found this bit of code incredibly helpful in managing some larger dash apps with many pages. One item I have yet to try with this is sending a JSON payload via post to some url. Thats next on my to-do list. . Responsive Layout . One of the things I like best about Dash is being able to easily transfer plotly charts I’ve created in Jupyter Notebook/Lab and put them on a dashboard by dcc.Graph. In general this is painless and works well, but it can feel clunky without a few adjustments to make the layout responsive. . There are a few ways to do this but here is my preference: . Use Dash Bootstrap Component and be sure your layout is encapsulated by a dbc.Container with fluid=True. | Be sure any dcc.Graph component is in a dbc.Col. By doing this the default behavior is for the plot to automatically resize in the horizontal direction based on the viewport (aka browser window). | For plots to resize vertically, use viewport units to define the height. For a plot that takes up 50% of the viewport screen: figure=go.Figure(layout_height=&#39;50vh&#39;). (This can also be used for width, but I prefer to let the dbc.Col handle that by setting the width argument there.) | Note: If your plot has layout_height and/or layout_width defined by pixels (passing an integer to these keywords assumes pixels) the plot will not resize correctly. . It should also be mentioned that if you do not desire responsive plots responsive=False can be set in the dcc.Graph component. . Remove the Plotly Logo . Another quick note on plots. If you’d like to strip the plotly logo off the tool bar of your plots set displaylogo=False. Heres a histogram plotting example from the dash-demo repo: . dcc.Graph(figure=plots.histogram(data[distribution_name]), style={&quot;height&quot;: &quot;80vh&quot;}, config={ &quot;displaylogo&quot;: False, }, ) . Callback Triggers . Callbacks become central to any interactive dashboard made with Dash. A single callback yields a single html object, it is not possible to have multiple callbacks that control the same object. To get around this, it can be helpful to have a single callback, with multiple inputs and track which input is triggered. Based on the input triggered, different logic or actions can be applied to the resulting object. . Here is the key: trigger = dash.callback_context.triggered. Inside of your call back function this trigger can tell you which input was changed and its new value. . @app.callback( Output(&quot;my-output-div&quot;,&quot;children&quot;), [Input(&quot;input_a&quot;, &quot;Value), Input(&quot;input_b&quot;,&quot;value&quot;)] ) def multiple_input_actions(value_a, value_b): trigger = dash.callback_context.triggered if trigger[0][&quot;propid&quot;] == &quot;input_a&quot;: # do something particular for input_a here return &quot;input_a triggered callback&quot; else: # do something else here for input_b return &quot;input_b triggered callback&quot; . Conclusion . Dash is great. There are a handful of alternatives (streamlit, panel, bokeh etc) they all have their place too, but I highly recommend dash if you need a professional looking dashboard but want to stop short of building a complete flask app. The above tips, snippets and strategies are things I use regularly and hopefully they’ll be helpful for others. If so, please let me know. Or, if you’ve got a better approach I’d love to hear that too. For a more complete example take a look at the dash-demo repo which includes examples of most of the items mentioned above. .",
            "url": "geostats.dev/python/plotly/dash/flask/dash%20bootstrap%20components/2020/11/26/dash-post.html",
            "relUrl": "/python/plotly/dash/flask/dash%20bootstrap%20components/2020/11/26/dash-post.html",
            "date": " • Nov 26, 2020"
        }
        
    
  
    
  
    
        ,"post2": {
            "title": "nbdev + GitHub Codespaces: A New Literate Programming Environment",
            "content": "Today, we are going to show you how to set up a literate programming environment, allowing you to use an IDE (VS Code) and an interactive computing environment (Jupyter), without leaving your browser, for free, in under 5 minutes. You’ll even see how VSCode and Jupyter work together automatically! But first, what is literate programming? And how did I go from skeptic to a zealot of literate programming? . Introduction . Literate programming is a programming paradigm introduced by Donald Knuth in which a computer program is given an explanation of its logic in a natural language, such as English, interspersed with snippets of macros and traditional source code, from which compilable source code can be generated. According to Knuth, literate programming provides higher-quality programs by forcing programmers to explicitly state the thoughts behind the program. This process makes poorly thought-out design decisions more obvious. Knuth also claims that literate programming provides a first-rate documentation system, which is not an add-on, but is grown naturally in the process of exposition of one’s thoughts during a program’s creation. 1 . When I first learned about literate programming, I was quite skeptical. For the longest time, I had wrongly equated Jupyter notebooks with literate programming. Indeed, Jupyter is a brilliant interactive computing system, which was awarded the Association of Computing Machinery (ACM) Software System Award, and is loved by many developers. However, Jupyter falls short of the literate programming paradigm for the following reasons:2 . It can be difficult to compile source code from notebooks. | It can be difficult to diff and use version control with notebooks because they are not stored in plain text. | It is not clear how to automatically generate documentation from notebooks. | It is not clear how to properly run tests suites when writing code in notebooks. | . My skepticism quickly evaporated when I began using nbdev, a project that extends notebooks to complete the literate programming ideal. I spent a month, full time, using nbdev while contributing to the python library fastcore, and can report that Donald Knuth was definitely onto something. The process of writing prose and tests alongside code forced me to deeply understand why the code does what it does, and to think deeply about its design. Furthermore, the reduced cognitive load and speed of iteration of having documentation, code, and tests in one location boosted my productivity to levels I have never before experienced as a software developer. Furthermore, I found that developing this way bolstered collaboration such that code reviews not only happened faster but were more meaningful. In short, nbdev may be the most profound productivity tool I have ever used. . As a teaser, look how easy it is to instantiate this literate programming environment, which includes a notebook, a docs site and an IDE with all dependencies pre-installed! :point_down: . . Features of nbdev . As discussed in the docs, nbdev provides the following features: . Searchable, hyperlinked documentation, which can be automatically hosted on GitHub Pages for free. | Python modules, following best practices such as automatically defining __all__ with your exported functions, classes, and variables. | Pip and Conda installers. | Tests defined directly in notebooks which run in parallel. This testing system has been thoroughly tested with GitHub Actions. | Navigate and edit your code in a standard text editor or IDE, and export any changes automatically back into your notebooks. | . Since you are in a notebook, you can also add charts, text, links, images, videos, etc, that are included automatically in the documentation of your library, along with standardized documentation generated automatically from your code. This site is an example of docs generated automatically by nbdev. . GitHub Codespaces . Thanks to Conda and nbdev_template, setting up a development environment with nbdev is far easier than it used to be. However, we realized it could be even easier, thanks to a new GitHub product called Codespaces. Codespaces is a fully functional development environment in your browser, accessible directly from GitHub, that provides the following features: . A full VS Code IDE. | An environment that has files from the repository mounted into the environment, along with your GitHub credentials. | A development environment with dependencies pre-installed, backed by Docker. | The ability to serve additional applications on arbitrary ports. For nbdev, we serve a Jupyter notebook server as well as a Jekyll based documentation site. | A shared file system, which facilitates editing code in one browser tab and rendering the results in another. | … and more. | Codespaces enables developers to immediately participate in a project without wasting time on DevOps or complicated setup steps. Most importantly, CodeSpaces with nbdev allows developers to quickly get started with creating their own software with literate programming. . A demo of nbdev + Codespaces . This demo uses the project fastai/fastcore, which was built with nbdev, as an example. First, we can navigate to this repo and launch a Codespace: . . If you are launching a fresh Codespace, it may take several minutes to set up. Once the environment is ready, we can verify that all dependencies we want are installed (in this case fastcore and nbdev): . . Additionally, we can serve an arbitrary number of applications on user-specified ports, which we can open through VSCode as shown below: . . In this case, these applications are a notebook and docs site. Changes to a notebook are reflected immediately in the data docs. Furthermore, we can use the cli command nbdev_build_lib to sync our notebooks with python modules. This functionality is shown below: . . This is amazing! With a click of a button, I was able to: . Launch an IDE with all dependencies pre-installed. | Launch two additional applications: a Jupyter Notebook server on port 8080 and a docs site on port 4000. | Automatically update the docs and modules every time I make a change to a Jupyter notebook. | This is just the tip of the iceberg. There are additional utilities for writing and executing tests, diffing notebooks, special flags for hiding, showing, and collapsing cells in the generated docs, as well as git hooks for automation. This and more functionality is covered in the nbdev docs. . Give It A Try For Yourself . To try out nbdev yourself, take this tutorial, which will walk you through everything you need to know. The tutorial also shows you how to use a repository template with the configuration files necessary to enable Codespaces with nbdev. . You Can Write Blogs With Notebooks, Too! . This blog post was written in fastpages which is also built on nbdev! We recommend fastpages if you want an easy way to blog with Jupyter notebooks. . Additional Resources . The GitHub Codepaces site. | The official docs for Codespaces. | The nbdev docs. | The nbdev GitHub repo. | fastpages: The project used to write this blog. | The GitHub repo fastai/fastcore, which is what we used in this blog post as an example. | . Wikipedia article: Literate Programming &#8617; . | This is not a criticism of Jupyter. Jupyter doesn’t claim to be a full literate programming system. However, people can sometimes (unfairly) judge Jupyter according to this criteria. &#8617; . |",
            "url": "geostats.dev/codespaces",
            "relUrl": "/codespaces",
            "date": " • Nov 2, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Getting Started with Python on Windows",
            "content": "Getting Started with Python on Windows . Reading python how-to guides, blogs and even tweets often gives the illusion that all things python take place entirely on Linux and Mac OS. I’m on windows 99% of the time when using python and happy to report it works great. There is a bit of a lingering stigma regarding python on windows because there was a time when getting python installed and setup on a windows machine was a quite a hassle. Those days are over. In fact, there are more windows users than mac users according to the JetBrains survey of 24k python users: . . Good news, that is absolutely no longer the case. However, I do think getting started with Python on windows can be a bit confusing. The goal of this post is to give a brief, but opinionated summary of how to go from never having touched python to running code in a jupyter notebook ASAP. . Where to get Python . Google something along the lines of “install python windows” and you’ll get python.org, closely followed by realpython.com and then a Microsoft page suggesting a python download from the Microsoft Appstore. Instead of these options, seek out Anaconda. This is a cross-platform solution to getting python installed and running with ease. . The Anaconda distribution includes: . The latest version of python (create an environment for other versions needed - more on that to come) | conda command line tool | A substantial set of pre-installed packages (particularly useful for scientific computing or data science) | Anaconda Navigator (for those that prefer a GUI to launch python based tools) | . Note: that if you desire a lighter weight version of Anaconda without the pre-installed packages consider miniconda . Starting Jupyter . Jupyter Notebooks and Jupyter Lab are an approachable way to get started writing some code for beginners, as well as a fantastic way to combine data visualization with text and blocks of code for more experienced users. . To get started either launch anaconda navigator from the start menu, or open a command line terminal, navigate to your working directory and open by either jupyter notebookor jupyter lab. . For some the command line terminal is unfamiliar or intimidating - instead jupyter notebook or lab can be launched from the file explorer with the same command (juptyer notebook or jupter lab) entered in the address bar: . . Virtual Environments with . A virtual environment can be thought of as an isolated install of python with a set of packages separate from the system (or user) -wide install. A virtual environment provides the freedom to work on projects that require a different version of python, or necessitate package(s) of different versions despite the latest and greatest that was installed with the Anaconda download. Additionally, a virtual environment can be replicated in another location or on another machine ensuring the ability to run your code wherever its needed. Shared below are some common conda commands for creating/using/cloning environments. If you need more info the conda documentation is excellent. . Environment Setup . Creating a new environment is simple . conda env create -n mynewenv (replacing mynewenv with a more appropriate name). . Anytime you want to use this environment, activate it . conda activate mynewenv . Once activated, anytime you call python or it will call the version specific to that environment, isolated from the global install. . Note: Using Jupyter (Notebook or Lab) within a Conda Environment requires an extra step. . If you want to use your conda environment in a jupyter notebook : . conda install ipykernel nb_conda_kernels . ipykernel provides everything you need to run jupyter from your new environment | nb_conda_kernels provides the added bonus of being able to access other conda environments on your system from within jupyter | . Copying an Environment . Ultimately, you’ll want to share your code with a friend or colleague to replicate some python workflow on a new machine or new data elsewhere. Using a Virtual Environment makes this easy. To replicate an environment, simply record all packages being used and their version number. To clone an environment on the same machine: . conda create --name myenvclone --clone myoldenv . Conda also provides a convenient way to export all the pertinent information to a YAML file. From within the environment you’d like to copy: . conda env export &gt; environment.yml . Now, take this environment.yml and recreate that exact same environment on a different machine: . conda env create -f environment.yml . Note: The YAML approach for saving environment information does not replace the standard requirements.txt that is common with pip. Instead, the conda approach is best suited for exactly the process described above: cloning an environment. Same name, all the same packages etc. requirements.txt is best suited for a list dependencies required for some package, stopping short of creating an entire environment. . Conclusion . Python works great on windows, don’t let anyone tell you otherwise. If you’re working on windows, especially in the realm of scientific computing or data science, Anaconda is highly recommended. Once you’ve got Anaconda set up and are comfortable with the basics, consider working a virtual environment for most things you’re doing. Does absolutely every bit of code need its own environment, definitely not but for any substantial project its highly recommended. .",
            "url": "geostats.dev/beginner/python/anaconda/conda/virtual%20environments/2020/10/08/python-on-windows.html",
            "relUrl": "/beginner/python/anaconda/conda/virtual%20environments/2020/10/08/python-on-windows.html",
            "date": " • Oct 8, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Pandas DataFrames with a GSLIB I/O Methods",
            "content": "Pandas is everywhere, for good reason. If you are using python to manipulate data, chances are you&#39;re using pandas. If you are using python for geoscience, you&#39;ve likely come across some weird FORTRAN generated file format that was a great idea in the 80s, but is a bit of a hassle for I/O operations. Pandas offers a large number of read/write methods, but occasionally some archaic file format comes along that can be a challenge (MODFLOW anyone?). But don&#39;t limit yourself to I/O operations, if there is additional functionality you desire from Pandas, you can extend dataframe, series or index functionality to do just that. If extending existing Pandas objects isn&#39;t enough, with a little extra effort and a few important details subclass and make your own DataFrame class. . Step 1: Reusable Function . In the last post about reading/writing GSLIB files, I shared a couple short snippets I use for reading/writing GEO-EAS (GSLIB) files to/from Pandas. As an example, I&#39;ll extend pandas to include these convenient read/write operations: . def write_gslib(self, filename:str): with open(filename, &quot;w&quot;) as f: f.write(&quot;GSLIB Example Data n&quot;) f.write(f&quot;{len(self._obj.columns)} n&quot;) f.write(&quot; n&quot;.join(self._obj.columns)+&quot; n&quot;) for row in df.itertuples(): row_data = &quot; t&quot;.join([f&quot;{i:.3f}&quot; for i in row[1:]]) f.write(f&quot;{row_data} n&quot;) def read_gslib(filename:str): with open(filename, &quot;r&quot;) as f: lines = f.readlines() ncols = int(lines[1].split()[0]) col_names = [lines[i+2].strip() for i in range(ncols)] df = pd.read_csv(filename, skiprows=ncols+2, delim_whitespace=True, names=col_names) return df . Step 2: pd.DataFrame Accessor . I&#39;m a big fan of this Pandas functionality. Just a simple decorator opens up the ability to add your own methods, properties etc. Here are the steps: . Make a class out your function(s). I&#39;ll call mine . GSLIBAccessor: . | class GSLIBAccessor: def __init__(self, pandas_obj): self._obj=(pandas_obj) def write_gslib(self, filename:str): with open(filename, &quot;w&quot;) as f: f.write(&quot;GSLIB Example Data n&quot;) f.write(f&quot;{len(self._obj.columns)} n&quot;) f.write(&quot; n&quot;.join(self._obj.columns)+&quot; n&quot;) for row in df.itertuples(): row_data = &quot; t&quot;.join([f&quot;{i:.3f}&quot; for i in row[1:]]) f.write(f&quot;{row_data} n&quot;) def read_gslib(filename:str): with open(filename, &quot;r&quot;) as f: lines = f.readlines() ncols = int(lines[1].split()[0]) col_names = [lines[i+2].strip() for i in range(ncols)] df = pd.read_csv(filename, skiprows=ncols+2, delim_whitespace=True, names=col_names) return df . In the __init__ include your DataFrame that this method will be operating on, pandas_obj in above snippet. | Add the decorator (Also I removed the _gslib suffix): | @pd.api.extensions.register_dataframe_accessor(&quot;gslib&quot;) class GSLIBAccessor: def __init__(self, pandas_obj): self._obj=(pandas_obj) def write(self, filename:str): with open(filename, &quot;w&quot;) as f: f.write(&quot;GSLIB Example Data n&quot;) f.write(f&quot;{len(self._obj.columns)} n&quot;) f.write(&quot; n&quot;.join(self._obj.columns)+&quot; n&quot;) for row in df.itertuples(): row_data = &quot; t&quot;.join([f&quot;{i:.3f}&quot; for i in row[1:]]) f.write(f&quot;{row_data} n&quot;) @staticmethod def read(filename:str): with open(filename, &quot;r&quot;) as f: lines = f.readlines() ncols = int(lines[1].split()[0]) col_names = [lines[i+2].strip() for i in range(ncols)] df = pd.read_csv(filename, skiprows=ncols+2, delim_whitespace=True, names=col_names) return df . Boom! Done! Thats it. Fantastic, right? . Now, the functionwrite_gslib is available as a DataFrame method at df.gslib.write() . There are plenty of GSLIB specific details to manage here - fill null values with -999.00, add grid definition in the file header etc, but the point here is this is a fast, easy, flexible way to add whatever functionality you need to the DataFrame class. . Reading in a dataframe by this approach works just fine as well. Keep in mind, the method created is associated with the DataFrame and won&#39;t be accessible at same place as the other pandas I/O operations like pd.read_csv, instead it will be at pd.DataFrame.gslib.read. . df = pd.DataFrame.gslib.read(&quot;data/example.dat&quot;) df.head() . x y z var . 0 0.723 | 0.564 | 0.785 | 2.853 | . 1 0.915 | 0.317 | 0.357 | 0.749 | . 2 0.346 | 0.484 | 0.690 | 0.786 | . 3 0.591 | 0.150 | 0.669 | 0.290 | . 4 0.157 | 0.332 | 0.006 | 1.777 | . To write out the file: . df.gslib.write(&quot;data/export_data.dat&quot;) . with open(&quot;data/export_data.dat&quot;, &quot;r&quot;) as f: for i in range(10): print(f.readline().strip()) . GSLIB Example Data 4 x y z var 0.723 0.564 0.785 2.853 0.915 0.317 0.357 0.749 0.346 0.484 0.690 0.786 0.591 0.150 0.669 0.290 . Step 3: Subclassing Pandas DataFrame . If forwhatever reason the decorator accessor approach isn&#39;t enough, you can always create your own class entirely and inheirit pd.DataFrame. This is a bit more work and there are a couple import details not to be missed. . Inheiritance . Inheiritance is a common aspect of OOP (object oriented programming), and is a topic that warrants a discussion all its own. Rather than get into that can of worms, if want some more details I&#39;d suggest Real Python: Inheiritance and Composition. In example, create a new class and inheirit all the good things that pd.DataFrame does, but add a few properities, methods etc. . class GSLIBDataFrame(pd.DataFrame): def __init__(self, data, *args, **kwargs): super().__init__(data=data, *args, **kwargs) def write(self, filename:str): with open(filename, &quot;w&quot;) as f: f.write(&quot;GSLIB Example Data n&quot;) f.write(f&quot;{len(self._obj.columns)} n&quot;) f.write(&quot; n&quot;.join(self._obj.columns)+&quot; n&quot;) for row in df.itertuples(): row_data = &quot; t&quot;.join([f&quot;{i:.3f}&quot; for i in row[1:]]) f.write(f&quot;{row_data} n&quot;) @classmethod def read(cls, filename:str): with open(filename, &quot;r&quot;) as f: lines = f.readlines() ncols = int(lines[1].split()[0]) col_names = [lines[i+2].strip() for i in range(ncols)] df = pd.read_csv(filename, skiprows=ncols+2, delim_whitespace=True, names=col_names) return cls(df) . This works and is quite similar to the accessor example but I do think this apporach would scale better should you have big plans for your DIY dataframes. Lets consider a couple issues shown below . df = GSLIBDataFrame.read(&quot;data/example.dat&quot;) returned_df = df.applymap(lambda x: x*2) type(returned_df) . pandas.core.frame.DataFrame . If you use some standard pandas operatione, it will return a regular pd.DataFrame not a GSLIBDataFrame - To manage this, the _constructor must be defined to override the method inheirted from pandas. . | Any other properites created, but be added to the metadata list so that they are passed on to results of manipulation. . | To demonstrate in addition to the previously defined methods, add a property favorite_column, though this name is nonsense this can be a useful approach for defining a specific column that defines categories, domains or coordinates. . class GSLIBDataFrame(pd.DataFrame): def __init__(self, data, favorite_column=None, *args, **kwargs): super().__init__(data=data, *args, **kwargs) self.favorite_column = favorite_column _metadata = [&quot;favorite_column&quot;] @property def _constructor(self): return GSLIBDataFrame def write(self, filename:str): with open(filename, &quot;w&quot;) as f: f.write(&quot;GSLIB Example Data n&quot;) f.write(f&quot;{len(self._obj.columns)} n&quot;) f.write(&quot; n&quot;.join(self._obj.columns)+&quot; n&quot;) for row in df.itertuples(): row_data = &quot; t&quot;.join([f&quot;{i:.3f}&quot; for i in row[1:]]) f.write(f&quot;{row_data} n&quot;) @classmethod def read(cls, filename:str, favorite_column:str): with open(filename, &quot;r&quot;) as f: lines = f.readlines() ncols = int(lines[1].split()[0]) col_names = [lines[i+2].strip() for i in range(ncols)] df = pd.read_csv(filename, skiprows=ncols+2, delim_whitespace=True, names=col_names) return cls(data=df, favorite_column=favorite_column) . Now, instead of returning a pd.DataFrame a GSLIBDataFrame is returned as a result of manipulation. . df = GSLIBDataFrame.read(&quot;data/example.dat&quot;, favorite_column=&quot;var&quot;) returned_df = df.applymap(lambda x: x*2) type(returned_df) . __main__.GSLIBDataFrame . Subclassing pd.DataFrame requires a bit more effort and has some quirks but in the long run might be worthwhile in cases where more than just a few methods and/or properites are going to be added to the class. If you&#39;re going down this road, have a look at the Pandas Documentation: Extending Pandas, much of what I shared here is paraphrased from their fantastic documentation. Code snippets posted as gists on github. .",
            "url": "geostats.dev/python/jupyter/pandas/gslib/oop/2020/09/17/extending-pandas.html",
            "relUrl": "/python/jupyter/pandas/gslib/oop/2020/09/17/extending-pandas.html",
            "date": " • Sep 17, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Importing and Exporting GSLIB (GEO-EAS) Files",
            "content": "Though a bit dated GSLIB remains the standard in many Geostatistical workflows, unfortunately the GSLIB data format can be a bit of hassle. The standard GSLIB aka GEO-EAS data format as described on gslib.com: . The first line in the file is taken as a title and is possibly transferred to output files. | The second line should be a numerical value specifying the number of numerical variables nvar in the data file. | The next nvar lines contain character identification labels and additional text (optional) that describe each variable. | The following lines, from nvar+3 until the end of the file, are considered as data points and must have nvar numerical values per line. Missing values are typically considered as large negative or positive numbers (e.g., less than -1.0e21 or greater than 1.0e21). The number of data will be the number of lines in the file minus nvar+2 minus the number of missing values. The programs read numerical values and not alphanumeric characters; alphanumeric variables may be transformed to integers or the source code modified. | . The header is informative, but not convienent for importing into Pandas. It should be noted that line #2 in the header can often contain grid definition information in addition to ncols, and in the case of multiple simulations nsim is commonly given after the grid definition (this is overlooked in the read/write functions to follow). . The goal here is just to provide a couple simple functions to save a little time for anyone who needs to do this. . Reading GSLIB data . Importing GSLIB data really happens in 2 steps. . 1. read the header 2. read all the data to a dataframe. . side note:I&#39;ve found skip_rows and delim_whitespace are useful when it comes to reading ASCII data from other scientific software (MODFLOW, PEST, TOUGH2 etc.) . def read_gslib(filename:str): with open(filename, &quot;r&quot;) as f: lines = f.readlines() ncols = int(lines[1].split()[0]) col_names = [lines[i+2].strip() for i in range(ncols)] df = pd.read_csv(filename, skiprows=ncols+2, delim_whitespace=True, names=col_names) return df . df = read_gslib(filename=&quot;data/example.dat&quot;) df.head() . x y z var . 0 0.723 | 0.564 | 0.785 | 2.853 | . 1 0.915 | 0.317 | 0.357 | 0.749 | . 2 0.346 | 0.484 | 0.690 | 0.786 | . 3 0.591 | 0.150 | 0.669 | 0.290 | . 4 0.157 | 0.332 | 0.006 | 1.777 | . Now go about your business analyzing data, making plots and doing all the other things python does well until you need re-export to GSLIB to run specific Geostatistical algorithm. . Writing a Pandas DataFrame to GSLIB Format . As with reading in the data, I&#39;m sure there are a number of ways this can be done. Below is one rather simple approach where I write the header than iterate over each row as a tuple. . If speed is a consideration when iterating over a pandas DataFrame use .itertuples its noticeably faster than .iterrows. . def write_gslib(df:pd.DataFrame, filename:str): with open(filename, &quot;w&quot;) as f: f.write(&quot;GSLIB Example Data n&quot;) f.write(f&quot;{len(df.columns)} n&quot;) f.write(&quot; n&quot;.join(df.columns)+&quot; n&quot;) for row in df.itertuples(): row_data = &quot; t&quot;.join([f&quot;{i:.3f}&quot; for i in row[1:]]) f.write(f&quot;{row_data} n&quot;) . write_gslib(df, &quot;data/exported_data.dat&quot;) . Now, just have a quick look at the file to be sure its correct: . with open(&quot;data/exported_data.dat&quot;,&quot;r&quot;) as f: for i in range(10): print(f.readline().strip()) . GSLIB Example Data 4 x y z var 0.723 0.564 0.785 2.853 0.915 0.317 0.357 0.749 0.346 0.484 0.690 0.786 0.591 0.150 0.669 0.290 . Really the whole purpose here is to have these functions readily available to copy/paste when you need them. .",
            "url": "geostats.dev/jupyter/gslib/pandas/geostatistics/python/2020/09/14/pandas-and-gslib.html",
            "relUrl": "/jupyter/gslib/pandas/geostatistics/python/2020/09/14/pandas-and-gslib.html",
            "date": " • Sep 14, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "First Post",
            "content": "Why? . In the last few years I’ve slowly started a list of things that were interesting, challenging or particularly useful that I’ve encountered working with python. The mental list turned into a written list and now a blog for a few reasons. . Its an opportunity to write down some of these tid-bits so I don’t forget them and have a reference next time something similar comes up. | Fastpages looked like a fun low barrier to entry blogging platform that makes it easy to share some code from a notebook, markdown or even a word file. | self-promotion. I’m generally terrible at this but every time I sign up for something new they ask for a github profile or personal website. If you’ve seen my github profile it needs some work… so here we are. | Does the world need another python blog? . Maybe, but probably not. . There are a lot great resources out there for learning python. Even so, I decided to start my own little page on the internet to share a few of the things I’ve learned and what I’m working on. I can’t promise code shared here will be absolutely perfect, but maybe it will be interesting and hopefully it will save you some time if you’re working on something similar. . Geoscience . My academic and professional background is in the geosciences (Geostatistics and Geology) and over the last 10 years I’ve found Python to be an amazing [nearly] do it all tool to automate and improve day to day tasks. My interests in geoscience focused on (but not limited to) subsurface modeling. In most cases this implies some knowledge and a limited amount of data from subsurface and building models to predict whats going in the areas without existing data. . Python . I’m not sure where I heard it, but I was once told “Python is the second best language for everything”, and I think thats true. In many cases there is another tool that might be better suited for a single specific task but as soon as you are connecting a few specific tasks into a larger workflow python outshines the other options as the best tool for the job. Python comes with drawbacks, out of the box it isn’t the fastest language out there but its fun to write, easy to read and has a rapidly growing set of tools for almost anything you can think of. . Goals . My goal is to add a post at least every few weeks. I don’t really want to get into the weeds on Geostatistics (but if thats of interest let me know!), I’d rather share some small ah ha moments and lessons learned the hard way using python. Hopefully these snippets will be useful to others outside of my own little niche in the geoscience world as well. .",
            "url": "geostats.dev/python/geoscience/about%20me/2020/09/13/first-post.html",
            "relUrl": "/python/geoscience/about%20me/2020/09/13/first-post.html",
            "date": " • Sep 13, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "fastcore: An Underrated Python Library",
            "content": ". Background . I recently embarked on a journey to sharpen my python skills: I wanted to learn advanced patterns, idioms, and techniques. I started with reading books on advanced Python, however, the information didn&#39;t seem to stick without having somewhere to apply it. I also wanted the ability to ask questions from an expert while I was learning -- which is an arrangement that is hard to find! That&#39;s when it occurred to me: What if I could find an open source project that has fairly advanced python code and write documentation and tests? I made a bet that if I did this it would force me to learn everything very deeply, and the maintainers would be appreciative of my work and be willing to answer my questions. . And that&#39;s exactly what I did over the past month! I&#39;m pleased to report that it has been the most efficient learning experience I&#39;ve ever experienced. I&#39;ve discovered that writing documentation forced me to deeply understand not just what the code does but also why the code works the way it does, and to explore edge cases while writing tests. Most importantly, I was able to ask questions when I was stuck, and maintainers were willing to devote extra time knowing that their mentorship was in service of making their code more accessible! It turns out the library I choose, fastcore is some of the most fascinating Python I have ever encountered as its purpose and goals are fairly unique. . For the uninitiated, fastcore is a library on top of which many fast.ai projects are built on. Most importantly, fastcore extends the python programming language and strives to eliminate boilerplate and add useful functionality for common tasks. In this blog post, I&#39;m going to highlight some of my favorite tools that fastcore provides, rather than sharing what I learned about python. My goal is to pique your interest in this library, and hopefully motivate you to check out the documentation after you are done to learn more! . Why fastcore is interesting . Get exposed to ideas from other languages without leaving python: I’ve always heard that it is beneficial to learn other languages in order to become a better programmer. From a pragmatic point of view, I’ve found it difficult to learn other languages because I could never use them at work. Fastcore extends python to include patterns found in languages as diverse as Julia, Ruby and Haskell. Now that I understand these tools I am motivated to learn other languages. | You get a new set of pragmatic tools: fastcore includes utilities that will allow you to write more concise expressive code, and perhaps solve new problems. | Learn more about the Python programming language: Because fastcore extends the python programming language, many advanced concepts are exposed during the process. For the motivated, this is a great way to see how many of the internals of python work. | A whirlwind tour through fastcore . Here are some things you can do with fastcore that immediately caught my attention. . . Making **kwargs transparent . Whenever I see a function that has the argument **kwargs, I cringe a little. This is because it means the API is obfuscated and I have to read the source code to figure out what valid parameters might be. Consider the below example: . def baz(a, b=2, c =3, d=4): return a + b + c def foo(c, a, **kwargs): return c + baz(a, **kwargs) inspect.signature(foo) . &lt;Signature (c, a, **kwargs)&gt; . Without reading the source code, it might be hard for me to know that foo also accepts and additional parameters b and d. We can fix this with delegates: . def baz(a, b=2, c =3, d=4): return a + b + c @delegates(baz) # this decorator will pass down keyword arguments from baz def foo(c, a, **kwargs): return c + baz(a, **kwargs) inspect.signature(foo) . &lt;Signature (c, a, b=2, d=4)&gt; . You can customize the behavior of this decorator. For example, you can have your cake and eat it too by passing down your arguments and also keeping **kwargs: . @delegates(baz, keep=True) def foo(c, a, **kwargs): return c + baz(a, **kwargs) inspect.signature(foo) . &lt;Signature (c, a, b=2, d=4, **kwargs)&gt; . You can also exclude arguments. For example, we exclude argument d from delegation: . def basefoo(a, b=2, c =3, d=4): pass @delegates(basefoo, but= [&#39;d&#39;]) # exclude `d` def foo(c, a, **kwargs): pass inspect.signature(foo) . &lt;Signature (c, a, b=2)&gt; . You can also delegate between classes: . class BaseFoo: def __init__(self, e, c=2): pass @delegates()# since no argument was passsed here we delegate to the superclass class Foo(BaseFoo): def __init__(self, a, b=1, **kwargs): super().__init__(**kwargs) inspect.signature(Foo) . &lt;Signature (a, b=1, c=2)&gt; . For more information, read the docs on delegates. . . Avoid boilerplate when setting instance attributes . Have you ever wondered if it was possible to avoid the boilerplate involved with setting attributes in __init__? . class Test: def __init__(self, a, b ,c): self.a, self.b, self.c = a, b, c . Ouch! That was painful. Look at all the repeated variable names. Do I really have to repeat myself like this when defining a class? Not Anymore! Checkout store_attr: . class Test: def __init__(self, a, b, c): store_attr() t = Test(5,4,3) assert t.b == 4 . You can also exclude certain attributes: . class Test: def __init__(self, a, b, c): store_attr(but=[&#39;c&#39;]) t = Test(5,4,3) assert t.b == 4 assert not hasattr(t, &#39;c&#39;) . There are many more ways of customizing and using store_attr than I highlighted here. Check out the docs for more detail. . P.S. you might be thinking that Python dataclasses also allow you to avoid this boilerplate. While true in some cases, store_attr is more flexible.1 . 1. For example, store_attr does not rely on inheritance, which means you won&#39;t get stuck using multiple inheritance when using this with your own classes. Also, unlike dataclasses, store_attr does not require python 3.7 or higher. Furthermore, you can use store_attr anytime in the object lifecycle, and in any location in your class to customize the behavior of how and when variables are stored.↩ . . Avoiding subclassing boilerplate . One thing I hate about python is the __super__().__init__() boilerplate associated with subclassing. For example: . class ParentClass: def __init__(self): self.some_attr = &#39;hello&#39; class ChildClass(ParentClass): def __init__(self): super().__init__() cc = ChildClass() assert cc.some_attr == &#39;hello&#39; # only accessible b/c you used super . We can avoid this boilerplate by using the metaclass PrePostInitMeta. We define a new class called NewParent that is a wrapper around the ParentClass: . class NewParent(ParentClass, metaclass=PrePostInitMeta): def __pre_init__(self, *args, **kwargs): super().__init__() class ChildClass(NewParent): def __init__(self):pass sc = ChildClass() assert sc.some_attr == &#39;hello&#39; . . Type Dispatch . Type dispatch, or Multiple dispatch, allows you to change the way a function behaves based upon the input types it receives. This is a prominent feature in some programming languages like Julia. For example, this is a conceptual example of how multiple dispatch works in Julia, returning different values depending on the input types of x and y: . collide_with(x::Asteroid, y::Asteroid) = ... # deal with asteroid hitting asteroid collide_with(x::Asteroid, y::Spaceship) = ... # deal with asteroid hitting spaceship collide_with(x::Spaceship, y::Asteroid) = ... # deal with spaceship hitting asteroid collide_with(x::Spaceship, y::Spaceship) = ... # deal with spaceship hitting spaceship . Type dispatch can be especially useful in data science, where you might allow different input types (i.e. Numpy arrays and Pandas dataframes) to a function that processes data. Type dispatch allows you to have a common API for functions that do similar tasks. . Unfortunately, Python does not support this out-of-the box. Fortunately, there is the @typedispatch decorator to the rescue. This decorator relies upon type hints in order to route inputs the correct version of the function: . @typedispatch def f(x:str, y:str): return f&#39;{x}{y}&#39; @typedispatch def f(x:np.ndarray): return x.sum() @typedispatch def f(x:int, y:int): return x+y . Below is a demonstration of type dispatch at work for the function f: . f(&#39;Hello &#39;, &#39;World!&#39;) . &#39;Hello World!&#39; . f(2,3) . 5 . f(np.array([5,5,5,5])) . 20 . There are limitations of this feature, as well as other ways of using this functionality that you can read about here. In the process of learning about typed dispatch, I also found a python library called multipledispatch made by Mathhew Rocklin (the creator of Dask). . After using this feature, I am now motivated to learn languages like Julia to discover what other paradigms I might be missing. . . A better version of functools.partial . functools.partial is a great utility that creates functions from other functions that lets you set default values. Lets take this function for example that filters a list to only contain values &gt;= val: . test_input = [1,2,3,4,5,6] def f(arr, val): &quot;Filter a list to remove any values that are less than val.&quot; return [x for x in arr if x &gt;= val] f(test_input, 3) . [3, 4, 5, 6] . You can create a new function out of this function using partial that sets the default value to 5: . filter5 = partial(f, val=5) filter5(test_input) . [5, 6] . One problem with partial is that it removes the original docstring and replaces it with a generic docstring: . filter5.__doc__ . &#39;partial(func, *args, **keywords) - new function with partial application n of the given arguments and keywords. n&#39; . fastcore.utils.partialler fixes this, and makes sure the docstring is retained such that the new API is transparent: . filter5 = partialler(f, val=5) filter5.__doc__ . &#39;Filter a list to remove any values that are less than val.&#39; . . Composition of functions . A technique that is pervasive in functional programming languages is function composition, whereby you chain a bunch of functions together to achieve some kind of result. This is especially useful when applying various data transformations. Consider a toy example where I have three functions: (1) Removes elements of a list less than 5 (from the prior section) (2) adds 2 to each number (3) sums all the numbers: . def add(arr, val): return [x + val for x in arr] def arrsum(arr): return sum(arr) # See the previous section on partialler add2 = partialler(add, val=2) transform = compose(filter5, add2, arrsum) transform([1,2,3,4,5,6]) . 15 . But why is this useful? You might me thinking, I can accomplish the same thing with: . arrsum(add2(filter5([1,2,3,4,5,6]))) . You are not wrong! However, composition gives you a convenient interface in case you want to do something like the following: . def fit(x, transforms:list): &quot;fit a model after performing transformations&quot; x = compose(*transforms)(x) y = [np.mean(x)] * len(x) # its a dumb model. Don&#39;t judge me return y # filters out elements &lt; 5, adds 2, then predicts the mean fit(x=[1,2,3,4,5,6], transforms=[filter5, add2]) . [7.5, 7.5] . For more information about compose, read the docs. . . A more useful __repr__ . In python, __repr__ helps you get information about an object for logging and debugging. Below is what you get by default when you define a new class. (Note: we are using store_attr, which was discussed earlier). . class Test: def __init__(self, a, b=2, c=3): store_attr() # `store_attr` was discussed previously Test(1) . &lt;__main__.Test at 0x7ffcd766cee0&gt; . We can use basic_repr to quickly give us a more sensible default: . class Test: def __init__(self, a, b=2, c=3): store_attr() __repr__ = basic_repr(&#39;a,b,c&#39;) Test(2) . Test(a=2, b=2, c=3) . . Monkey Patching With A Decorator . It can be convenient to monkey patch with a decorator, which is especially helpful when you want to patch an external library you are importing. We can use the decorator @patch from fastcore.foundation along with type hints like so: . class MyClass(int): pass @patch def func(self:MyClass, a): return self+a mc = MyClass(3) . Now, MyClass has an additional method named func: . mc.func(10) . 13 . Still not convinced? I&#39;ll show you another example of this kind of patching in the next section. . . A better pathlib.Path . When you see these extensions to pathlib.path you won&#39;t ever use vanilla pathlib again! A number of additional methods have been added to pathlib, such as: . Path.readlines: same as with open(&#39;somefile&#39;, &#39;r&#39;) as f: f.readlines() | Path.read: same as with open(&#39;somefile&#39;, &#39;r&#39;) as f: f.read() | Path.save: saves file as pickle | Path.load: loads pickle file | Path.ls: shows the contents of the path as a list. | etc. | . Read more about this here. Here is a demonstration of ls: . from fastcore.utils import * from pathlib import Path p = Path(&#39;.&#39;) p.ls() # you don&#39;t get this with vanilla Pathlib.Path!! . (#7) [Path(&#39;2020-09-01-fastcore.ipynb&#39;),Path(&#39;README.md&#39;),Path(&#39;fastcore_imgs&#39;),Path(&#39;2020-02-20-test.ipynb&#39;),Path(&#39;.ipynb_checkpoints&#39;),Path(&#39;2020-02-21-introducing-fastpages.ipynb&#39;),Path(&#39;my_icons&#39;)] . Wait! What&#39;s going on here? We just imported pathlib.Path - why are we getting this new functionality? Thats because we imported the fastcore.utils module, which patches this module via the @patch decorator discussed earlier. Just to drive the point home on why the @patch decorator is useful, I&#39;ll go ahead and add another method to Path right now: . @patch def fun(self:Path): return &quot;This is fun!&quot; p.fun() . &#39;This is fun!&#39; . That is magical, right? I know! That&#39;s why I&#39;m writing about it! . . An Even More Concise Way To Create Lambdas . Self, with an uppercase S, is an even more concise way to create lambdas that are calling methods on an object. For example, let&#39;s create a lambda for taking the sum of a Numpy array: . arr=np.array([5,4,3,2,1]) f = lambda a: a.sum() assert f(arr) == 15 . You can use Self in the same way: . f = Self.sum() assert f(arr) == 15 . Let&#39;s create a lambda that does a groupby and max of a Pandas dataframe: . import pandas as pd df=pd.DataFrame({&#39;Some Column&#39;: [&#39;a&#39;, &#39;a&#39;, &#39;b&#39;, &#39;b&#39;, ], &#39;Another Column&#39;: [5, 7, 50, 70]}) f = Self.groupby(&#39;Some Column&#39;).mean() f(df) . Another Column . Some Column . a 6 | . b 60 | . Read more about Self in the docs). . . Notebook Functions . These are simple but handy, and allow you to know whether or not code is executing in a Jupyter Notebook, Colab, or an Ipython Shell: . from fastcore.imports import in_notebook, in_colab, in_ipython in_notebook(), in_colab(), in_ipython() . (True, False, True) . This is useful if you are displaying certain types of visualizations, progress bars or animations in your code that you may want to modify or toggle depending on the environment. . . A Drop-In Replacement For List . You might be pretty happy with Python&#39;s list. This is one of those situations that you don&#39;t know you needed a better list until someone showed one to you. Enter L, a list like object with many extra goodies. . The best way I can describe L is to pretend that list and numpy had a pretty baby: . define a list (check out the nice __repr__ that shows the length of the list!) . L(1,2,3) . (#3) [1,2,3] . Shuffle a list: . p = L.range(20).shuffle() p . (#20) [8,7,5,12,14,16,2,15,19,6...] . Index into a list: . p[2,4,6] . (#3) [5,14,2] . L has sensible defaults, for example appending an element to a list: . 1 + L(2,3,4) . (#4) [1,2,3,4] . There is much more L has to offer. Read the docs to learn more. . But Wait ... There&#39;s More! . There are more things I would like to show you about fastcore, but there is no way they would reasonably fit into a blog post. Here is a list of some of my favorite things that I didn&#39;t demo in this blog post: . Utilities . The Basics section contain many shortcuts to perform common tasks or provide an additional interface to what standard python provides. . mk_class: quickly add a bunch of attributes to a class | wrap_class: add new methods to a class with a simple decorator | groupby: similar to Scala&#39;s groupby | merge: merge dicts | fasttuple: a tuple on steroids | Infinite Lists: useful for padding and testing | chunked: for batching and organizing stuff | . Multiprocessing . The Multiprocessing section extends python&#39;s multiprocessing library by offering features like: . progress bars | ability to pause to mitigate race conditions with external services | processing things in batches on each worker, ex: if you have a vectorized operation to perform in chunks | . Functional Programming . The functional programming section is my favorite part of this library. . maps: a map that also composes functions | mapped: A more robust map | using_attr: compose a function that operates on an attribute | . Transforms . Transforms is a collection of utilities for creating data transformations and associated pipelines. These transformation utilities build upon many of the building blocks discussed in this blog post. . Further Reading . It should be noted that you should read the main page of the docs first, followed by the section on tests to fully understand the documentation. . The fastcore documentation site. | The fastcore GitHub repo. | Blog post on delegation. | . Shameless plug: fastpages . This blog post was written entirely in a Jupyter Notebook, which GitHub automatically converted into to a blog post! Sound interesting? Check out fastpages. .",
            "url": "geostats.dev/fastcore/",
            "relUrl": "/fastcore/",
            "date": " • Sep 1, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "GitHub Actions: Providing Data Scientists With New Superpowers",
            "content": "What Superpowers? . Hi, I’m Hamel Husain. I’m a machine learning engineer at GitHub. Recently, GitHub released a new product called GitHub Actions, which has mostly flown under the radar in the machine learning and data science community as just another continuous integration tool. . Recently, I’ve been able to use GitHub Actions to build some very unique tools for Data Scientists, which I want to share with you today. Most importantly, I hope to get you excited about GitHub Actions, and the promise it has for giving you new superpowers as a Data Scientist. Here are two projects I recently built with Actions that show off its potential: . fastpages . fastpages is an automated, open-source blogging platform with enhanced support for Jupyter notebooks. You save your notebooks, markdown, or Word docs into a directory on GitHub, and they automatically become blog posts. Read the announcement below: . We&#39;re launching `fastpages`, a platform which allows you to host a blog for free, with no ads. You can blog with @ProjectJupyter notebooks, @office Word, directly from @github&#39;s markdown editor, etc.Nothing to install, &amp; setup is automated!https://t.co/dNSA0oQUrN . &mdash; Jeremy Howard (@jeremyphoward) February 24, 2020 Machine Learning Ops . Wouldn’t it be cool if you could invoke a chatbot natively on GitHub to test your machine learning models on the infrastructure of your choice (GPUs), log all the results, and give you a rich report back in a pull request so that everyone could see the results? You can with GitHub Actions! . Consider the below annotated screenshot of this Pull Request: . . A more in-depth explanation about the above project can be viewed in this video: . Using GitHub Actions for machine learning workflows is starting to catch on. Julien Chaumond, CTO of Hugging Face, says: . GitHub Actions are great because they let us do CI on GPUs (as most of our users use the library on GPUs not on CPUs), on our own infra! 1 . Additionally, you can host a GitHub Action for other people so others can use parts of your workflow without having to re-create your steps. I provide examples of this below. . A Gentle Introduction To GitHub Actions . What Are GitHub Actions? . GitHub Actions allow you to run arbitrary code in response to events. Events are activities that happen on GitHub such as: . Opening a pull request | Making an issue comment | Labeling an issue | Creating a new branch | … and many more | . When an event is created, the GitHub Actions context is hydrated with a payload containing metadata for that event. Below is an example of a payload that is received when an issue is created: . { &quot;action&quot;: &quot;created&quot;, &quot;issue&quot;: { &quot;id&quot;: 444500041, &quot;number&quot;: 1, &quot;title&quot;: &quot;Spelling error in the README file&quot;, &quot;user&quot;: { &quot;login&quot;: &quot;Codertocat&quot;, &quot;type&quot;: &quot;User&quot;, }, &quot;labels&quot;: [ { &quot;id&quot;: 1362934389, &quot;node_id&quot;: &quot;MDU6TGFiZWwxMzYyOTM0Mzg5&quot;, &quot;name&quot;: &quot;bug&quot;, } ], &quot;body&quot;: &quot;It looks like you accidently spelled &#39;commit&#39; with two &#39;t&#39;s.&quot; } . This functionality allows you to respond to various events on GitHub in an automated way. In addition to this payload, GitHub Actions also provide a plethora of variables and environment variables that afford easy to access metadata such as the username and the owner of the repo. Additionally, other people can package useful functionality into an Action that other people can inherit. For example, consider the below Action that helps you publish python packages to PyPi: . The Usage section describes how this Action can be used: . - name: Publish a Python distribution to PyPI uses: pypa/gh-action-pypi-publish@master with: user: __token__ password: ${{ secrets.pypi_password }} . This Action expects two inputs: user and a password. You will notice that the password is referencing a variable called secrets, which is a variable that contains an encrypted secret that you can upload to your GitHub repository. There are thousands of Actions (that are free) for a wide variety of tasks that can be discovered on the GitHub Marketplace. The ability to inherit ready-made Actions in your workflow allows you to accomplish complex tasks without implementing all of the logic yourself. Some useful Actions for those getting started are: . actions/checkout: Allows you to quickly clone the contents of your repository into your environment, which you often want to do. This does a number of other things such as automatically mount your repository’s files into downstream Docker containers. | mxschmitt/action-tmate: Proivdes a way to debug Actions interactively. This uses port forwarding to give you a terminal in the browser that is connected to your Actions runner. Be careful not to expose sensitive information if you use this. | actions/github-script: Gives you a pre-authenticated ocotokit.js client that allows you to interact with the GitHub API to accomplish almost any task on GitHub automatically. Only these endpoints are supported (for example, the secrets endpoint is not in that list). | . In addition to the aforementioned Actions, it is helpful to go peruse the official GitHub Actions docs before diving in. . Example: A fastpages Action Workflow . The best to way familiarize yourself with Actions is by studying examples. Let’s take a look at the Action workflow that automates the build of fastpages (the platform used to write this blog post). . Part 1: Define Workflow Triggers . First, we define triggers in ci.yaml. Like all Actions workflows, this is a YAML file located in the .github/workflows directory of the GitHub repo. . The top of this YAML file looks like this: . name: CI on: push: branches: - master pull_request: . This means that this workflow is triggered on either a push or pull request event. Furthermore, push events are filtered such that only pushes to the master branch will trigger the workflow, whereas all pull requests will trigger this workflow. It is important to note that pull requests opened from forks will have read-only access to the base repository and cannot access any secrets for security reasons. The reason for defining the workflow in this way is we wanted to trigger the same workflow to test pull requests as well as build and deploy the website when a PR is merged into master. This will be clarified as we step through the rest of the YAML file. . Part 2: Define Jobs . Next, we define jobs (there is only one in this workflow). Per the docs: . A workflow run is made up of one or more jobs. Jobs run in parallel by default. . jobs: build-site: if: ( github.event.commits[0].message != &#39;Initial commit&#39; ) || github.run_number &gt; 1 runs-on: ubuntu-latest steps: . The keyword build-site is the name of your job and you can name it whatever you want. In this case, we have a conditional if statement that dictates if this job should be run or not. We are trying to ensure that this workflow does not run when the first commit to a repo is made with the message ‘Initial commit’. The first variable in the if statement, github.event, contains a json payload of the event that triggered this workflow. When developing workflows, it is helpful to print this variable in order to inspect its structure, which you can accomplish with the following YAML: . - name: see payload run: | echo &quot;PAYLOAD: n${PAYLOAD} n&quot; env: PAYLOAD: ${{ toJSON(github.event) }} . Note: the above step is only for debugging and is not currently in the workflow. . toJson is a handy function that returns a pretty-printed JSON representation of the variable. The output is printed directly in the logs contained in the Actions tab of your repo. In this example, printing the payload for a push event will look like this (truncated for brevity): . { &quot;ref&quot;: &quot;refs/tags/simple-tag&quot;, &quot;before&quot;: &quot;6113728f27ae8c7b1a77c8d03f9ed6e0adf246&quot;, &quot;created&quot;: false, &quot;deleted&quot;: true, &quot;forced&quot;: false, &quot;base_ref&quot;: null, &quot;commits&quot;: [ { &quot;message&quot;: &quot;updated README.md&quot;, &quot;author&quot;: &quot;hamelsmu&quot; }, ], &quot;head_commit&quot;: null, } . Therefore, the variable github.event.commits[0].message will retrieve the first commit message in the array of commits. Since we are looking for situations where there is only one commit, this logic suffices. The second variable in the if statement, github.run_number is a special variable in Actions which: . [is a] unique number for each run of a particular workflow in a repository. This number begins at 1 for the workflow’s first run, and increments with each new run. This number does not change if you re-run the workflow run. . Therefore, the if statement introduced above: . if: ( github.event.commits[0].message != &#39;Initial commit&#39; ) || github.run_number &gt; 1 . Allows the workflow to run when the commit message is “Initial commit” as long as it is not the first commit. ( || is a logical or operator). . Finally, the line runs-on: ubuntu-latest specifies the host operating system that your workflows will run in. . Part 3: Define Steps . Per the docs: . A job contains a sequence of tasks called steps. Steps can run commands, run setup tasks, or run an Action in your repository, a public repository, or an Action published in a Docker registry. Not all steps run Actions, but all Actions run as a step. Each step runs in its own process in the runner environment and has access to the workspace and filesystem. Because steps run in their own process, changes to environment variables are not preserved between steps. GitHub provides built-in steps to set up and complete a job. . Below are the first two steps in our workflow: . - name: Copy Repository Contents uses: actions/checkout@master with: persist-credentials: false - name: convert notebooks and word docs to posts uses: ./_action_files . The first step creates a copy of your repository in the Actions file system, with the help of the utility action/checkout. This utility only fetches the last commit by default and saves files into a directory (whose path is stored in the environment variable GITHUB_WORKSPACE that is accessible by subsequent steps in your job. The second step runs the fastai/fastpages Action, which converts notebooks and word documents to blog posts automatically. In this case, the syntax: . uses: ./_action_files . is a special case where the pre-made GitHub Action we want to run happens to be defined in the same repo that runs this workflow. This syntax allows us to test changes to this pre-made Action when evaluating PRs by referencing the directory in the current repository that defines that pre-made Action. Note: Building pre-made Actions is beyond the scope of this tutorial. . The next three steps in our workflow are defined below: . - name: setup directories for Jekyll build run: | rm -rf _site sudo chmod -R 777 . - name: Jekyll build uses: docker://fastai/fastpages-jekyll with: args: bash -c &quot;gem install bundler &amp;&amp; jekyll build -V&quot; env: JEKYLL_ENV: &#39;production&#39; - name: copy CNAME file into _site if CNAME exists run: | sudo chmod -R 777 _site/ cp CNAME _site/ 2&gt;/dev/null || : . The step named setup directories for Jekyll build executes shell commands that remove the _site folder in order to get rid of stale files related to the page we want to build, as well as grant permissions to all the files in our repo to subsequent steps. . The step named Jekyll build executes a docker container hosted by the Jekyll community on Dockerhub called jekyll/jekyll. For those not familiar with Docker, see this tutorial. The name of this container is called fastai/fastpages-jekyll because I’m adding some additional dependencies to jekyll/jekyll and hosting those on my DockerHub account for faster build times2. The args parameter allows you to execute arbitrary commands with the Docker container by overriding the CMD instruction in the Dockerfile. We use this Docker container hosted on Dockerhub so we don’t have to deal with installing and configuring all of the complicated dependencies for Jekyll. The files from our repo are already available in the Actions runtime due to the first step in this workflow, and are mounted into this Docker container automatically for us. In this case, we are running the command jekyll build, which builds our website and places relevant assets them into the _site folder. For more information about Jekyll, read the official docs. Finally, the env parameter allows me to pass an environment variable into the Docker container. . The final command above copies a CNAME file into the _site folder, which we need for the custom domain https://fastpages.fast.ai. Setting up custom domains are outside the scope of this article. . The final step in our workflow is defined below: . - name: Deploy if: github.event_name == &#39;push&#39; uses: peaceiris/actions-gh-pages@v3 with: deploy_key: ${{ secrets.SSH_DEPLOY_KEY }} publish_dir: ./_site . The statement . if: github.event_name == &#39;push&#39; . uses the variable github.event_name to ensure this step only runs when a push event ( in this case only pushes to the master branch trigger this workflow) occur. . This step deploys the fastpages website by copying the contents of the _site folder to the root of the gh-pages branch, which GitHub Pages uses for hosting. This step uses the peaceiris/actions-gh-pages Action, pinned at version 3. Their README describes various options and inputs for this Action. . Conclusion . We hope that this has shed some light on how we use GitHub Actions to automate fastpages. While we only covered one workflow above, we hope this provides enough intuition to understand the other workflows in fastpages. We have only scratched the surface of GitHub Actions in this blog post, but we provide other materials below for those who want to dive in deeper. We have not covered how to host an Action for other people, but you can start with these docs to learn more. . Still confused about how GitHub Actions could be used for Data Science? Here are some ideas of things you can build: . Jupyter Widgets that trigger GitHub Actions to perform various tasks on GitHub via the repository dispatch event | Integration with Pachyderm for data versioning. | Integration with your favorite cloud machine learning services, such Sagemaker, Azure ML or GCP’s AI Platform. | . Related Materials . GitHub Actions official documentation | Hello world Docker Action: A template to demonstrate how to build a Docker Action for other people to use. | Awesome Actions: A curated list of interesting GitHub Actions by topic. | A tutorial on Docker for Data Scientists. | . Getting In Touch . Please feel free to get in touch with us on Twitter: . Hamel Husain @HamelHusain | Jeremy Howard @jeremyphoward | . . Footnotes . You can see some of Hugging Face’s Actions workflows for machine learning on GitHub &#8617; . | These additional dependencies are defined here, which uses the “jekyll build” command to add ruby dedpendencies from the Gemfile located at the root of the repo. Additionally, this docker image is built by another Action workflow defined here. &#8617; . |",
            "url": "geostats.dev/actions/markdown/2020/03/06/fastpages-actions.html",
            "relUrl": "/actions/markdown/2020/03/06/fastpages-actions.html",
            "date": " • Mar 6, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "Introducing fastpages",
            "content": ". We are very pleased to announce the immediate availability of fastpages. fastpages is a platform which allows you to create and host a blog for free, with no ads and many useful features, such as: . Create posts containing code, outputs of code (which can be interactive), formatted text, etc directly from Jupyter Notebooks; for instance see this great example post from Scott Hawley. Notebook posts support features such as: Interactive visualizations made with Altair remain interactive. | Hide or show cell input and output. | Collapsable code cells that are either open or closed by default. | Define the Title, Summary and other metadata via a special markdown cells | Ability to add links to Colab and GitHub automatically. | . | Create posts, including formatting and images, directly from Microsoft Word documents. | Create and edit Markdown posts entirely online using GitHub&#39;s built-in markdown editor. | Embed Twitter cards and YouTube videos. | Categorization of blog posts by user-supplied tags for discoverability. | ... and much more | . fastpages relies on Github pages for hosting, and Github Actions to automate the creation of your blog. The setup takes around three minutes, and does not require any technical knowledge or expertise. Due to built-in automation of fastpages, you don&#39;t have to fuss with conversion scripts. All you have to do is save your Jupyter notebook, Word document or markdown file into a specified directory and the rest happens automatically. Infact, this blog post is written in a Jupyter notebook, which you can see with the &quot;View on GitHub&quot; link above. . fast.ai have previously released a similar project called fast_template, which is even easier to set up, but does not support automatic creation of posts from Microsoft Word or Jupyter notebooks, including many of the features outlined above. . Because fastpages is more flexible and extensible, we recommend using it where possible. fast_template may be a better option for getting folks blogging who have no technical expertise at all, and will only be creating posts using Github&#39;s integrated online editor. . Setting Up Fastpages . The setup process of fastpages is automated with GitHub Actions, too! Upon creating a repo from the fastpages template, a pull request will automatically be opened (after ~ 30 seconds) configuring your blog so it can start working. The automated pull request will greet you with instructions like this: . . All you have to do is follow these instructions (in the PR you receive) and your new blogging site will be up and running! . Jupyter Notebooks &amp; Fastpages . In this post, we will cover special features that fastpages provides for Jupyter notebooks. You can also write your blog posts with Word documents or markdown in fastpages, which contain many, but not all the same features. . Options via FrontMatter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # Title &gt; Awesome summary - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . All of the above settings are enabled in this post, so you can see what they look like! . the summary field (preceeded by &gt;) will be displayed under your title, and will also be used by social media to display as the description of your page. | toc: setting this to true will automatically generate a table of contents | badges: setting this to true will display Google Colab and GitHub links on your blog post. | comments: setting this to true will enable comments. See these instructions for more details. | author this will display the authors names. | categories will allow your post to be categorized on a &quot;Tags&quot; page, where readers can browse your post by categories. | . Markdown front matter is formatted similarly to notebooks. The differences between the two can be viewed on the fastpages README. . Code Folding . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . If you want to completely hide cells (not just collapse them), read these instructions. . Interactive Charts With Altair . Interactive visualizations made with Altair remain interactive! . We leave this below cell unhidden so you can enjoy a preview of syntax highlighting in fastpages, which uses the Dracula theme. . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;IMDB_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget IMDB_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | 6.1 | . 1 First Love, Last Rites | 10876.0 | 300000.0 | 6.9 | . 2 I Married a Strange Person | 203134.0 | 250000.0 | 6.8 | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | NaN | . 4 Slam | 1087521.0 | 1000000.0 | 3.4 | . Other Features . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Images w/Captions . You can include markdown images with captions like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Of course, the caption is optional. . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . More Examples . This tutorial contains more examples of what you can do with notebooks. . How fastpages Converts Notebooks to Blog Posts . fastpages uses nbdev to power the conversion process of Jupyter Notebooks to blog posts. When you save a notebook into the /_notebooks folder of your repository, GitHub Actions applies nbdev against those notebooks automatically. The same process occurs when you save Word documents or markdown files into the _word or _posts directory, respectively. . We will discuss how GitHub Actions work in a follow up blog post. . Resources &amp; Next Steps . We highly encourage you to start blogging with fastpages! Some resources that may be helpful: . fastpages repo - this is where you can go to create your own fastpages blog! | Fastai forums - nbdev &amp; blogging category. You can ask questions about fastpages here, as well as suggest new features. | nbdev: this project powers the conversion of Jupyter notebooks to blog posts. | . If you end up writing a blog post using fastpages, please let us know on Twitter: @jeremyphoward, @HamelHusain. .",
            "url": "geostats.dev/fastpages/jupyter/2020/02/21/introducing-fastpages.html",
            "relUrl": "/fastpages/jupyter/2020/02/21/introducing-fastpages.html",
            "date": " • Feb 21, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "geostats.dev/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "geostats.dev/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  
    
        ,"post12": {
            "title": "Microsoft Word Example Post",
            "content": "When writing a blog post with Microsoft Word – the filename becomes the title. In this case the file name is “2020-01-01-Microsoft-Word-Example-Post.docx”. . There is minimal support for Word documents in fastpages compared to Jupyter notebooks. Some known limitations: . alt text in Word documents are not yet supported by fastpages, and will break links to images. . | You can only specify front matter for Word documents globally. See the README for more details. . | . For greater control over the content produced from Word documents, you will need to convert Word to markdown files manually. You can follow the steps in this blog post, which walk you through how to use pandoc to do the conversion. Note: If you wish to customize your Word generated blog post in markdown, make sure you delete your Word document from the _word directory so your markdown file doesn’t get overwritten! . If your primary method of writing blog posts is Word documents, and you plan on always manually editing Word generated markdown files, you are probably better off using fast_template instead of fastpages. . The material below is a reproduction of this blog post, and serves as an illustrative example. . Maintaining a healthy open source project can entail a huge amount of toil. Popular projects often have orders of magnitude more users and episodic contributors opening issues and PRs than core maintainers capable of handling these issues. . Consider this graphic prepared by the NumFOCUS foundation showing the number of maintainers for three widely used scientific computing projects: . . We can see that across these three projects, there is a very low ratio maintainers to users. Fixing this problem is not an easy task and likely requires innovative solutions to address the economics as well as tools. . Due to its recent momentum and popularity, Kubeflow suffers from a similar fate as illustrated by the growth of new issues opened: . . Source: “TensorFlow World 2019, Automating Your Developer Workflow With ML” . Coincidentally, while building out end to end machine learning examples for Kubeflow, we built two examples using publicly available GitHub data: GitHub Issue Summarization and Code Search. While these tutorials were useful for demonstrating components of Kubeflow, we realized that we could take this a step further and build concrete data products that reduce toil for maintainers. . This is why we started the project kubeflow/code-intelligence, with the goals of increasing project velocity and health using data driven tools. Below are two projects we are currently experimenting with : . Issue Label Bot: This is a bot that automatically labels GitHub issues using Machine Learning. This bot is a GitHub App that was originally built for Kubeflow but is now also used by several large open source projects. The current version of this bot only applies a very limited set of labels, however we are currently A/B testing new models that allow personalized labels. Here is a blog post discussing this project in more detail. . | Issue Triage GitHub Action: to compliment the Issue Label Bot, we created a GitHub Action that automatically adds / removes Issues to the Kubeflow project board tracking issues needing triage. . | Together these projects allow us to reduce the toil of triaging issues. The GitHub Action makes it much easier for the Kubeflow maintainers to track issues needing triage. With the label bot we have taken the first steps in using ML to replace human intervention. We plan on using features extracted by ML to automate more steps in the triage process to further reduce toil. . Building Solutions with GitHub Actions . One of the premises of Kubeflow is that a barrier to building data driven, ML powered solutions is getting models into production and integrated into a solution. In the case of building models to improve OSS project health, that often means integrating with GitHub where the project is hosted. . We are really excited by GitHub’s newly released feature GitHub Actions because we think it will make integrating ML with GitHub much easier. . For simple scripts, like the issue triage script, GitHub actions make it easy to automate executing the script in response to GitHub events without having to build and host a GitHub app. . To automate adding/removing issues needing triage to a Kanban board we wrote a simple python script that interfaces with GitHub’s GraphQL API to modify issues. . As we continue to iterate on ML Models to further reduce toil, GitHub Actions will make it easy to leverage Kubeflow to put our models into production faster. A number of prebuilt GitHub Actions make it easy to create Kubernetes resources in response to GitHub events. For example, we have created GitHub Actions to launch Argo Workflows. This means once we have a Kubernetes job or workflow to perform inference we can easily integrate the model with GitHub and have the full power of Kubeflow and Kubernetes (eg. GPUs). We expect this will allow us to iterate much faster compared to building and maintaining GitHub Apps. . Call To Action . We have a lot more work to do in order to achieve our goal of reducing the amount of toil involved in maintaining OSS projects. If your interested in helping out here’s a couple of issues to get started: . Help us create reports that pull and visualize key performance indicators (KPI). https://github.com/kubeflow/code-intelligence/issues/71 . We have defined our KPI here: issue #19 | . | Combine repo specific and non-repo specific label predictions: https://github.com/kubeflow/code-intelligence/issues/70 . | . In addition to the aforementioned issues we welcome contributions for these other issues in our repo. .",
            "url": "geostats.dev/2020/01/01/Microsoft-Word-Example-Post.html",
            "relUrl": "/2020/01/01/Microsoft-Word-Example-Post.html",
            "date": " • Jan 1, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi, I’m Eric - Geostatistician &amp; Python Developer. . I started my career as Geologist in the in mining and exploration field. After a bit of numerical modeling and light introduction to Python I continued my studies at the University of Alberta’s Centre for Computational Geostatistics (CCG). My own research with Dr. Clayton Deutsch was focused on Prediction of Local Uncertainty and Localization. . My interest and experience in Python started with utilitarian scripts to string together a few FORTRAN programs. Since then I’ve gained an appreciation for Python and programming in a broader context - Machine Learning, Data Visualization, Reproducible/Shareable Workflows in Jupyter Notebooks and just a touch of Web Dev. .",
          "url": "geostats.dev/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
      ,"page3": {
          "title": "Resources",
          "content": "Learn Python . Real Python - Originally Michael Herman (TestDriven.io) now owned/run by Dan Bader, Real Python provides tutorials in both text, video and course format on all things python. I’ve learned a great deal from Real Python and highly recommend them. . | Talk Python To Me, Audio seems like a strange medium to learn anything code related but trust me this podcast is great. If you’ve got a background in something other than web development/computer science Talk Python To Me is a great way to get exposed to and learn from the wider world of python. Michael Kennedy does a great job. . | . Learn Geostatistics . GeostatsGuy aka Michael Pyrcz is a Professor at the University of Texas at Austin teaching Geostatistics and Machine Learning. Many (maybe all?) of his lectures are available on YouTube and they’re great for everyone beginner to expert. . | LazyModelingCrew This a group of somewhat recent graduates from the Centre for Computational Geostatistics at the University of Alberta. They’ve recently started sharing some easily digestible Geostatistics lessons as blog posts. I especially recommend the variogram focused posts for anyone new to geostats. . | Geostatistics Lessons Thoughtfully put together lessons for some of the more complex aspects of Geostatics. Mostly supported by the Centre for Computational Geostatics at the University of Alberta. . | . Tools, Packages etc for Geostatistics and Geoscience . AR2GAS . First a plug for the product I work on day to day: AR2GAS is c++ backed software with a Python API that can be run on your local machine, on a local cluster or on the cloud for geostatics on a regular or unstructured grid (or even gridless!). I think we’re doing great work and if you’re interested in learning more please reach out. . Open Source Geostatistics . There are a number of open source python packages available for Geostatistics using Python but my experience with them is limited and I can’t speak to how “production ready” they are. At some point I’d like to do a little compare/contrast/benchmark with a few of these. . SGeMS . SGeMS is a GUI based software for Geostatistics created at Stanford University. If you prefer clicking buttons to writing code this is a great option for free software. | . GSLIB . GSLIB is the original Geostatics Library of software created by Clayton Deutsch that has grown over the years. Its not fancy, but it works and you can always script a few programs together with subprocess. | . Python(ish) . The (ish) because a few of these use FORTRAN or C++ for the heavy lifting. . Pygeostat - Supported by Centre for Computational Geostatistics. Note much of the computation relies heavily on GSLIB FORTRAN programs. | GeostatsGuy - Michael Pyrcz’s python package for geostats. There are 2 components, one is pure python the other relies on GSLIB FORTRAN programs. | High Performance Geostatistics Library written in c++ with a python API - looks interesting and they give some benchmarks showing its substantially faster than GSLIB. | . I don’t know much about these other offerings but they have a number of stars on github so there are some people out there using them. . GSTOOls - Last commit 6 months ago.. | SciKit-Gstat - maybe still maintained? Last commit was 5 months ago. | .",
          "url": "geostats.dev/resources/",
          "relUrl": "/resources/",
          "date": ""
      }
      
  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "geostats.dev/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}